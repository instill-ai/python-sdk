# generated by datamodel-codegen:
#   filename:  airbyte_definitions.json

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Union

from . import OAuth2


class AWSRegion(Enum):
    """
    AWS Region of the SQS Queue
    """

    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


@dataclass
class Amazonsqs:
    access_key: Optional[str]
    destination: str
    message_body_key: Optional[str]
    message_delay: Optional[int]
    message_group_id: Optional[str]
    queue_url: str
    region: AWSRegion
    secret_key: Optional[str]


class CredentialsTitle(Enum):
    """
    Name of the credentials
    """

    IAM_Role = 'IAM Role'


@dataclass
class IAMRole:
    """
    Choose How to Authenticate to AWS.
    """

    credentials_title: CredentialsTitle
    role_arn: str


class CredentialsTitle1(Enum):
    """
    Name of the credentials
    """

    IAM_User = 'IAM User'


@dataclass
class IAMUser:
    """
    Choose How to Authenticate to AWS.
    """

    aws_access_key_id: str
    aws_secret_access_key: str
    credentials_title: CredentialsTitle1


class CompressionCodecOptional(Enum):
    """
    The compression algorithm used to compress data.
    """

    UNCOMPRESSED = 'UNCOMPRESSED'
    GZIP = 'GZIP'


class FormatType(Enum):
    JSONL = 'JSONL'


@dataclass
class JSONLinesNewlineDelimitedJSON:
    """
    Format of the data output.
    """

    format_type: FormatType
    compression_codec: Optional[
        CompressionCodecOptional
    ] = CompressionCodecOptional.UNCOMPRESSED


class CompressionCodecOptional1(Enum):
    """
    The compression algorithm used to compress data.
    """

    UNCOMPRESSED = 'UNCOMPRESSED'
    SNAPPY = 'SNAPPY'
    GZIP = 'GZIP'
    ZSTD = 'ZSTD'


class FormatType1(Enum):
    Parquet = 'Parquet'


@dataclass
class ParquetColumnarStorage:
    """
    Format of the data output.
    """

    format_type: FormatType1
    compression_codec: Optional[
        CompressionCodecOptional1
    ] = CompressionCodecOptional1.SNAPPY


class ChooseHowToPartitionData(Enum):
    """
    Partition data by cursor fields when a cursor field is a date
    """

    NO_PARTITIONING = 'NO PARTITIONING'
    DATE = 'DATE'
    YEAR = 'YEAR'
    MONTH = 'MONTH'
    DAY = 'DAY'
    YEAR_MONTH = 'YEAR/MONTH'
    YEAR_MONTH_DAY = 'YEAR/MONTH/DAY'


class S3BucketRegion(Enum):
    """
    The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


@dataclass
class Awsdatalake:
    aws_account_id: Optional[str]
    bucket_name: str
    bucket_prefix: Optional[str]
    credentials: Union[IAMRole, IAMUser]
    destination: str
    format: Optional[Union[JSONLinesNewlineDelimitedJSON, ParquetColumnarStorage]]
    lakeformation_database_default_tag_key: Optional[str]
    lakeformation_database_default_tag_values: Optional[str]
    lakeformation_database_name: str
    region: S3BucketRegion
    glue_catalog_float_as_decimal: Optional[bool] = False
    lakeformation_governed_tables: Optional[bool] = False
    partitioning: Optional[
        ChooseHowToPartitionData
    ] = ChooseHowToPartitionData.NO_PARTITIONING


class NormalizationFlattening(Enum):
    """
    Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
    """

    No_flattening = 'No flattening'
    Root_level_flattening = 'Root level flattening'


@dataclass
class CSVCommaSeparatedValues:
    """
    Output data format
    """

    flattening: NormalizationFlattening
    format_type: str


@dataclass
class JSONLinesNewlineDelimitedJSON1:
    """
    Output data format
    """

    format_type: str


@dataclass
class Azureblobstorage:
    azure_blob_storage_account_key: str
    azure_blob_storage_account_name: str
    azure_blob_storage_container_name: Optional[str]
    destination: str
    format: Union[CSVCommaSeparatedValues, JSONLinesNewlineDelimitedJSON1]
    azure_blob_storage_endpoint_domain_name: Optional[str] = 'blob.core.windows.net'
    azure_blob_storage_output_buffer_size: Optional[int] = 5
    azure_blob_storage_spill_size: Optional[int] = 500


class DatasetLocation(Enum):
    """
    The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
    """

    US = 'US'
    EU = 'EU'
    asia_east1 = 'asia-east1'
    asia_east2 = 'asia-east2'
    asia_northeast1 = 'asia-northeast1'
    asia_northeast2 = 'asia-northeast2'
    asia_northeast3 = 'asia-northeast3'
    asia_south1 = 'asia-south1'
    asia_south2 = 'asia-south2'
    asia_southeast1 = 'asia-southeast1'
    asia_southeast2 = 'asia-southeast2'
    australia_southeast1 = 'australia-southeast1'
    australia_southeast2 = 'australia-southeast2'
    europe_central1 = 'europe-central1'
    europe_central2 = 'europe-central2'
    europe_north1 = 'europe-north1'
    europe_southwest1 = 'europe-southwest1'
    europe_west1 = 'europe-west1'
    europe_west2 = 'europe-west2'
    europe_west3 = 'europe-west3'
    europe_west4 = 'europe-west4'
    europe_west6 = 'europe-west6'
    europe_west7 = 'europe-west7'
    europe_west8 = 'europe-west8'
    europe_west9 = 'europe-west9'
    europe_west12 = 'europe-west12'
    me_central1 = 'me-central1'
    me_central2 = 'me-central2'
    me_west1 = 'me-west1'
    northamerica_northeast1 = 'northamerica-northeast1'
    northamerica_northeast2 = 'northamerica-northeast2'
    southamerica_east1 = 'southamerica-east1'
    southamerica_west1 = 'southamerica-west1'
    us_central1 = 'us-central1'
    us_east1 = 'us-east1'
    us_east2 = 'us-east2'
    us_east3 = 'us-east3'
    us_east4 = 'us-east4'
    us_east5 = 'us-east5'
    us_south1 = 'us-south1'
    us_west1 = 'us-west1'
    us_west2 = 'us-west2'
    us_west3 = 'us-west3'
    us_west4 = 'us-west4'


@dataclass
class HMACKey:
    """
    An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
    """

    credential_type: str
    hmac_key_access_id: str
    hmac_key_secret: str


class GCSTmpFilesAfterwardProcessing(Enum):
    """
    This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
    """

    Delete_all_tmp_files_from_GCS = 'Delete all tmp files from GCS'
    Keep_all_tmp_files_in_GCS = 'Keep all tmp files in GCS'


@dataclass
class GCSStaging:
    """
    <i>(recommended)</i> Writes large batches of records to a file, uploads the file to GCS, then uses COPY INTO to load your data into BigQuery. Provides best-in-class speed, reliability and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
    """

    credential: HMACKey
    gcs_bucket_name: str
    gcs_bucket_path: str
    method: str
    keep_files_in_gcs_bucket: Optional[
        GCSTmpFilesAfterwardProcessing
    ] = GCSTmpFilesAfterwardProcessing.Delete_all_tmp_files_from_GCS


@dataclass
class StandardInserts:
    """
    <i>(not recommended)</i> Direct loading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In all other cases, you should use GCS staging.
    """

    method: str


class TransformationQueryRunType(Enum):
    """
    Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries donâ€™t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
    """

    interactive = 'interactive'
    batch = 'batch'


@dataclass
class Bigquery:
    credentials_json: Optional[str]
    dataset_id: str
    dataset_location: DatasetLocation
    destination: str
    loading_method: Optional[Union[GCSStaging, StandardInserts]]
    project_id: str
    raw_data_dataset: Optional[str]
    big_query_client_buffer_size_mb: Optional[int] = 15
    disable_type_dedupe: Optional[bool] = False
    transformation_priority: Optional[
        TransformationQueryRunType
    ] = TransformationQueryRunType.interactive


@dataclass
class Cassandra:
    address: str
    destination: str
    keyspace: str
    password: str
    port: int
    username: str
    datacenter: Optional[str] = 'datacenter1'
    replication: Optional[int] = 1


class Mode(Enum):
    azure_openai = 'azure_openai'


@dataclass
class AzureOpenAI:
    """
    Use the Azure-hosted OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    api_base: str
    deployment: str
    mode: Mode
    openai_key: str


class Mode1(Enum):
    openai = 'openai'


@dataclass
class OpenAI:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    mode: Mode1
    openai_key: str


class Mode2(Enum):
    cohere = 'cohere'


@dataclass
class Cohere:
    """
    Use the Cohere API to embed text.
    """

    cohere_key: str
    mode: Mode2


class Mode3(Enum):
    from_field = 'from_field'


@dataclass
class FromField:
    """
    Use a field in the record as the embedding. This is useful if you already have an embedding for your data and want to store it in the vector store.
    """

    dimensions: int
    field_name: str
    mode: Mode3


class Mode4(Enum):
    fake = 'fake'


@dataclass
class Fake:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Mode4


class Mode5(Enum):
    openai_compatible = 'openai_compatible'


@dataclass
class OpenAICompatible:
    """
    Use a service that's compatible with the OpenAI API to embed text.
    """

    base_url: str
    dimensions: int
    mode: Mode5
    api_key: Optional[str] = ''
    model_name: Optional[str] = 'text-embedding-ada-002'


class Mode6(Enum):
    no_embedding = 'no_embedding'


@dataclass
class ChromaDefaultEmbeddingFunction:
    """
    Do not calculate embeddings. Chromadb uses the sentence transfomer (https://www.sbert.net/index.html) as a default if an embedding function is not defined. Note that depending on your hardware, calculating embeddings locally can be very slow and is mostly suited for prototypes.
    """

    mode: Optional[Mode6] = Mode6.no_embedding


class Mode7(Enum):
    persistent_client = 'persistent_client'


@dataclass
class PersistentClientMode:
    """
    Configure Chroma to save and load from your local machine
    """

    path: str
    mode: Optional[Mode7] = Mode7.persistent_client


class Mode8(Enum):
    http_client = 'http_client'


@dataclass
class ClientServerMode:
    """
    Authenticate using username and password (suitable for self-managed Chroma clusters)
    """

    host: str
    port: int
    ssl: bool
    mode: Optional[Mode8] = Mode8.http_client
    password: Optional[str] = ''
    username: Optional[str] = ''


@dataclass
class Indexing:
    """
    Indexing configuration
    """

    auth_method: Union[PersistentClientMode, ClientServerMode]
    collection_name: str


@dataclass
class FieldNameMappingConfigModel:
    from_field: str
    to_field: str


class Mode9(Enum):
    separator = 'separator'


@dataclass
class BySeparator:
    """
    Split the text by the list of separators until the chunk size is reached, using the earlier mentioned separators where possible. This is useful for splitting text fields by paragraphs, sentences, words, etc.
    """

    mode: Mode9
    keep_separator: Optional[bool] = False
    separators: Optional[List[str]] = field(
        default_factory=lambda: ['"\\n\\n"', '"\\n"', '" "', '""']
    )


class Mode10(Enum):
    markdown = 'markdown'


@dataclass
class ByMarkdownHeader:
    """
    Split the text by Markdown headers down to the specified header level. If the chunk size fits multiple sections, they will be combined into a single chunk.
    """

    mode: Mode10
    split_level: Optional[int] = 1


class Language(Enum):
    """
    Split code in suitable places based on the programming language
    """

    cpp = 'cpp'
    go = 'go'
    java = 'java'
    js = 'js'
    php = 'php'
    proto = 'proto'
    python = 'python'
    rst = 'rst'
    ruby = 'ruby'
    rust = 'rust'
    scala = 'scala'
    swift = 'swift'
    markdown = 'markdown'
    latex = 'latex'
    html = 'html'
    sol = 'sol'


class Mode11(Enum):
    code = 'code'


@dataclass
class ByProgrammingLanguage:
    """
    Split the text by suitable delimiters based on the programming language. This is useful for splitting code into chunks.
    """

    language: Language
    mode: Mode11


@dataclass
class ProcessingConfigModel:
    chunk_size: int
    text_splitter: Optional[Union[BySeparator, ByMarkdownHeader, ByProgrammingLanguage]]
    chunk_overlap: Optional[int] = 0
    field_name_mappings: Optional[List[FieldNameMappingConfigModel]] = field(
        default_factory=lambda: []
    )
    metadata_fields: Optional[List[str]] = field(default_factory=lambda: [])
    text_fields: Optional[List[str]] = field(default_factory=lambda: [])


@dataclass
class Chroma:
    """
    The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
    as well as to provide type safety for the configuration passed to the destination.

    The configuration model is composed of four parts:
    * Processing configuration
    * Embedding configuration
    * Indexing configuration
    * Advanced configuration

    Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
    """

    destination: str
    embedding: Union[
        AzureOpenAI,
        OpenAI,
        Cohere,
        FromField,
        Fake,
        OpenAICompatible,
        ChromaDefaultEmbeddingFunction,
    ]
    indexing: Indexing
    processing: ProcessingConfigModel
    omit_raw_text: Optional[bool] = False


@dataclass
class NoTunnel:
    """
    Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
    """

    tunnel_method: str


@dataclass
class SSHKeyAuthentication:
    """
    Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
    """

    ssh_key: str
    tunnel_host: str
    tunnel_method: str
    tunnel_port: int
    tunnel_user: str


@dataclass
class PasswordAuthentication:
    """
    Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.
    """

    tunnel_host: str
    tunnel_method: str
    tunnel_port: int
    tunnel_user: str
    tunnel_user_password: str


@dataclass
class Clickhouse:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    ssl: Optional[bool] = False


@dataclass
class Convex:
    access_key: str
    deployment_url: str
    destination: str


@dataclass
class Comma:
    """
    The character delimiting individual cells in the CSV data.
    """

    delimiter: str


@dataclass
class Semicolon:
    """
    The character delimiting individual cells in the CSV data.
    """

    delimiter: str


@dataclass
class Pipe:
    """
    The character delimiting individual cells in the CSV data.
    """

    delimiter: str


@dataclass
class Tab:
    """
    The character delimiting individual cells in the CSV data.
    """

    delimiter: str


@dataclass
class Space:
    """
    The character delimiting individual cells in the CSV data.
    """

    delimiter: str


@dataclass
class Csv:
    delimiter_type: Optional[Union[Comma, Semicolon, Pipe, Tab, Space]]
    destination: str
    destination_path: str


@dataclass
class Cumulio:
    api_host: str
    api_key: str
    api_token: str
    destination: str


@dataclass
class Databend:
    database: str
    destination: str
    host: str
    password: Optional[str]
    username: str
    port: Optional[int] = 443
    table: Optional[str] = 'default'


@dataclass
class FieldRecommendedManagedTables:
    """
    Storage on which the delta lake is built.
    """

    data_source_type: str


class S3BucketRegion1(Enum):
    """
    The region of the S3 staging bucket to use if utilising a copy strategy.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


@dataclass
class AmazonS3:
    """
    Storage on which the delta lake is built.
    """

    data_source_type: str
    file_name_pattern: Optional[str]
    s3_access_key_id: str
    s3_bucket_name: str
    s3_bucket_path: str
    s3_bucket_region: S3BucketRegion1
    s3_secret_access_key: str


@dataclass
class AzureBlobStorage:
    """
    Storage on which the delta lake is built.
    """

    azure_blob_storage_account_name: str
    azure_blob_storage_container_name: str
    azure_blob_storage_sas_token: str
    data_source_type: str
    azure_blob_storage_endpoint_domain_name: Optional[str] = 'blob.core.windows.net'


@dataclass
class Databricks:
    accept_terms: bool
    data_source: Union[FieldRecommendedManagedTables, AmazonS3, AzureBlobStorage]
    database: Optional[str]
    databricks_http_path: str
    databricks_personal_access_token: str
    databricks_server_hostname: str
    destination: str
    databricks_port: Optional[str] = '443'
    enable_schema_evolution: Optional[bool] = False
    purge_staging_data: Optional[bool] = True
    schema_: Optional[str] = 'default'


@dataclass
class Doris:
    database: str
    destination: str
    host: str
    httpport: int
    password: Optional[str]
    queryport: int
    username: str


@dataclass
class Duckdb:
    destination: str
    destination_path: str
    motherduck_api_key: Optional[str]
    schema_: Optional[str]


class DynamoDBRegion(Enum):
    """
    The region of the DynamoDB.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


@dataclass
class Dynamodb:
    access_key_id: str
    destination: str
    dynamodb_region: DynamoDBRegion
    dynamodb_table_name_prefix: str
    secret_access_key: str
    dynamodb_endpoint: Optional[str] = ''


class LoggingType(Enum):
    FirstN = 'FirstN'


@dataclass
class FirstNEntries:
    """
    Log first N entries per stream.
    """

    logging_type: LoggingType
    max_entry_count: float


class LoggingType1(Enum):
    EveryNth = 'EveryNth'


@dataclass
class EveryNThEntry:
    """
    For each stream, log every N-th entry with a maximum cap.
    """

    logging_type: LoggingType1
    max_entry_count: float
    nth_entry_to_log: float


class LoggingType2(Enum):
    RandomSampling = 'RandomSampling'


@dataclass
class RandomSampling:
    """
    For each stream, randomly log a percentage of the entries with a maximum cap.
    """

    logging_type: LoggingType2
    max_entry_count: float
    sampling_ratio: float
    seed: Optional[float]


@dataclass
class Logging:
    """
    The type of destination to be used
    """

    logging_config: Union[FirstNEntries, EveryNThEntry, RandomSampling]
    test_destination_type: str


@dataclass
class Silent:
    """
    The type of destination to be used
    """

    test_destination_type: str


@dataclass
class Throttled:
    """
    The type of destination to be used
    """

    millis_per_record: int
    test_destination_type: str


@dataclass
class Failing:
    """
    The type of destination to be used
    """

    num_messages: int
    test_destination_type: str


@dataclass
class E2etest:
    destination: str
    test_destination: Union[Logging, Silent, Throttled, Failing]


@dataclass
class None1:
    """
    No authentication will be used
    """

    method: str


@dataclass
class ApiKeySecret:
    """
    Use a api key and secret combination to authenticate
    """

    apiKeyId: str
    apiKeySecret: str
    method: str


@dataclass
class UsernamePassword:
    """
    Basic auth header with a username and password
    """

    method: str
    password: str
    username: str


@dataclass
class Elasticsearch:
    authenticationMethod: Optional[Union[None1, ApiKeySecret, UsernamePassword]]
    ca_certificate: Optional[str]
    destination: str
    endpoint: str
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    upsert: Optional[bool] = True


@dataclass
class Exasol:
    certificateFingerprint: Optional[str]
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    schema_: str
    username: str


@dataclass
class SQLInserts:
    """
    Loading method used to select the way data will be uploaded to Firebolt
    """

    method: str


@dataclass
class ExternalTableViaS3:
    """
    Loading method used to select the way data will be uploaded to Firebolt
    """

    aws_key_id: str
    aws_key_secret: str
    method: str
    s3_bucket: str
    s3_region: str


@dataclass
class Firebolt:
    account: Optional[str]
    database: str
    destination: str
    engine: Optional[str]
    host: Optional[str]
    loading_method: Optional[Union[SQLInserts, ExternalTableViaS3]]
    password: str
    username: str


@dataclass
class Firestore:
    credentials_json: Optional[str]
    destination: str
    project_id: str


class CredentialType(Enum):
    HMAC_KEY = 'HMAC_KEY'


@dataclass
class HMACKey1:
    """
    An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
    """

    credential_type: CredentialType
    hmac_key_access_id: str
    hmac_key_secret: str


class Codec(Enum):
    no_compression = 'no compression'


@dataclass
class NoCompression:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec


class Codec1(Enum):
    Deflate = 'Deflate'


@dataclass
class Deflate:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec1
    compression_level: Optional[int] = 0


class Codec2(Enum):
    bzip2 = 'bzip2'


@dataclass
class Bzip2:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec2


class Codec3(Enum):
    xz = 'xz'


@dataclass
class Xz:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec3
    compression_level: Optional[int] = 6


class Codec4(Enum):
    zstandard = 'zstandard'


@dataclass
class Zstandard:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec4
    compression_level: Optional[int] = 3
    include_checksum: Optional[bool] = False


class Codec5(Enum):
    snappy = 'snappy'


@dataclass
class Snappy:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec5


class FormatType2(Enum):
    Avro = 'Avro'


@dataclass
class AvroApacheAvro:
    """
    Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
    """

    compression_codec: Union[NoCompression, Deflate, Bzip2, Xz, Zstandard, Snappy]
    format_type: FormatType2


class CompressionType(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression1:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType] = CompressionType.No_Compression


class CompressionType1(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType1] = CompressionType1.GZIP


class Normalization(Enum):
    """
    Whether the input JSON data should be normalized (flattened) in the output CSV. Please refer to docs for details.
    """

    No_flattening = 'No flattening'
    Root_level_flattening = 'Root level flattening'


class FormatType3(Enum):
    CSV = 'CSV'


@dataclass
class CSVCommaSeparatedValues1:
    """
    Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
    """

    compression: Optional[Union[NoCompression1, GZIP]]
    format_type: FormatType3
    flattening: Optional[Normalization] = Normalization.No_flattening


class CompressionType2(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression2:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType2] = CompressionType2.No_Compression


class CompressionType3(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP1:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType3] = CompressionType3.GZIP


class FormatType4(Enum):
    JSONL = 'JSONL'


@dataclass
class JSONLinesNewlineDelimitedJSON2:
    """
    Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
    """

    compression: Optional[Union[NoCompression2, GZIP1]]
    format_type: FormatType4


class CompressionCodec(Enum):
    """
    The compression algorithm used to compress data pages.
    """

    UNCOMPRESSED = 'UNCOMPRESSED'
    SNAPPY = 'SNAPPY'
    GZIP = 'GZIP'
    LZO = 'LZO'
    BROTLI = 'BROTLI'
    LZ4 = 'LZ4'
    ZSTD = 'ZSTD'


class FormatType5(Enum):
    Parquet = 'Parquet'


@dataclass
class ParquetColumnarStorage1:
    """
    Output data format. One of the following formats must be selected - <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro">AVRO</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas">PARQUET</a> format, <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table">CSV</a> format, or <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table">JSONL</a> format.
    """

    format_type: FormatType5
    block_size_mb: Optional[int] = 128
    compression_codec: Optional[CompressionCodec] = CompressionCodec.UNCOMPRESSED
    dictionary_encoding: Optional[bool] = True
    dictionary_page_size_kb: Optional[int] = 1024
    max_padding_size_mb: Optional[int] = 8
    page_size_kb: Optional[int] = 1024


class GCSBucketRegion(Enum):
    """
    Select a Region of the GCS Bucket. Read more <a href="https://cloud.google.com/storage/docs/locations">here</a>.
    """

    northamerica_northeast1 = 'northamerica-northeast1'
    northamerica_northeast2 = 'northamerica-northeast2'
    us_central1 = 'us-central1'
    us_east1 = 'us-east1'
    us_east4 = 'us-east4'
    us_west1 = 'us-west1'
    us_west2 = 'us-west2'
    us_west3 = 'us-west3'
    us_west4 = 'us-west4'
    southamerica_east1 = 'southamerica-east1'
    southamerica_west1 = 'southamerica-west1'
    europe_central2 = 'europe-central2'
    europe_north1 = 'europe-north1'
    europe_west1 = 'europe-west1'
    europe_west2 = 'europe-west2'
    europe_west3 = 'europe-west3'
    europe_west4 = 'europe-west4'
    europe_west6 = 'europe-west6'
    asia_east1 = 'asia-east1'
    asia_east2 = 'asia-east2'
    asia_northeast1 = 'asia-northeast1'
    asia_northeast2 = 'asia-northeast2'
    asia_northeast3 = 'asia-northeast3'
    asia_south1 = 'asia-south1'
    asia_south2 = 'asia-south2'
    asia_southeast1 = 'asia-southeast1'
    asia_southeast2 = 'asia-southeast2'
    australia_southeast1 = 'australia-southeast1'
    australia_southeast2 = 'australia-southeast2'
    asia = 'asia'
    eu = 'eu'
    us = 'us'
    asia1 = 'asia1'
    eur4 = 'eur4'
    nam4 = 'nam4'


@dataclass
class Gcs:
    credential: HMACKey1
    destination: str
    format: Union[
        AvroApacheAvro,
        CSVCommaSeparatedValues1,
        JSONLinesNewlineDelimitedJSON2,
        ParquetColumnarStorage1,
    ]
    gcs_bucket_name: str
    gcs_bucket_path: str
    gcs_bucket_region: Optional[GCSBucketRegion] = GCSBucketRegion.us


@dataclass
class AuthenticationViaGoogleOAuth:
    """
    Google API Credentials for connecting to Google Sheets and Google Drive APIs
    """

    client_id: str
    client_secret: str
    refresh_token: str


@dataclass
class Googlesheets:
    credentials: AuthenticationViaGoogleOAuth
    destination: str
    spreadsheet_id: str


class CatalogType(Enum):
    Hive = 'Hive'


@dataclass
class HiveCatalogUseApacheHiveMetaStore:
    """
    Catalog config of Iceberg.
    """

    catalog_type: CatalogType
    hive_thrift_uri: str
    database: Optional[str] = 'default'


class CatalogType1(Enum):
    Hadoop = 'Hadoop'


@dataclass
class HadoopCatalogUseHierarchicalFileSystemsAsSameAsStorageConfig:
    """
    A Hadoop catalog doesnâ€™t need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename.
    """

    catalog_type: CatalogType1
    database: Optional[str] = 'default'


class CatalogType2(Enum):
    Jdbc = 'Jdbc'


@dataclass
class JdbcCatalogUseRelationalDatabase:
    """
    Using a table in a relational database to manage Iceberg tables through JDBC. Read more <a href="https://iceberg.apache.org/docs/latest/jdbc/">here</a>. Supporting: PostgreSQL
    """

    catalog_type: CatalogType2
    jdbc_url: Optional[str]
    password: Optional[str]
    username: Optional[str]
    catalog_schema: Optional[str] = 'public'
    database: Optional[str] = 'public'
    ssl: Optional[bool] = False


class CatalogType3(Enum):
    Rest = 'Rest'


@dataclass
class RESTCatalog:
    """
    The RESTCatalog connects to a REST server at the specified URI
    """

    catalog_type: CatalogType3
    rest_credential: Optional[str]
    rest_token: Optional[str]
    rest_uri: str


class FileStorageFormat(Enum):
    Parquet = 'Parquet'
    Avro = 'Avro'


@dataclass
class FileFormat:
    """
    File format of Iceberg storage.
    """

    format: FileStorageFormat
    auto_compact: Optional[bool] = False
    compact_target_file_size_in_mb: Optional[int] = 100
    flush_batch_size: Optional[int] = 10000


class S3BucketRegion2(Enum):
    """
    The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


class StorageType(Enum):
    S3 = 'S3'


@dataclass
class S3:
    """
    S3 object storage
    """

    access_key_id: str
    s3_warehouse_uri: str
    secret_access_key: str
    storage_type: StorageType
    s3_bucket_region: Optional[S3BucketRegion2] = ''
    s3_endpoint: Optional[str] = ''
    s3_path_style_access: Optional[bool] = True


class StorageType1(Enum):
    MANAGED = 'MANAGED'


@dataclass
class ServerManaged:
    """
    Server-managed object storage
    """

    managed_warehouse_name: str
    storage_type: StorageType1


@dataclass
class Iceberg:
    catalog_config: Union[
        HiveCatalogUseApacheHiveMetaStore,
        HadoopCatalogUseHierarchicalFileSystemsAsSameAsStorageConfig,
        JdbcCatalogUseRelationalDatabase,
        RESTCatalog,
    ]
    destination: str
    format_config: FileFormat
    storage_config: Union[S3, ServerManaged]


class ACKs(Enum):
    """
    The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent.
    """

    field_0 = '0'
    field_1 = '1'
    all = 'all'


class ClientDNSLookup(Enum):
    """
    Controls how the client uses DNS lookups. If set to use_all_dns_ips, connect to each returned IP address in sequence until a successful connection is established. After a disconnection, the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the hostname again. If set to resolve_canonical_bootstrap_servers_only, resolve each bootstrap address into a list of canonical names. After the bootstrap phase, this behaves the same as use_all_dns_ips. If set to default (deprecated), attempt to connect to the first IP address returned by the lookup, even if the lookup returns multiple IP addresses.
    """

    default = 'default'
    use_all_dns_ips = 'use_all_dns_ips'
    resolve_canonical_bootstrap_servers_only = (
        'resolve_canonical_bootstrap_servers_only'
    )
    use_all_dns_ips_1 = 'use_all_dns_ips'


class CompressionType4(Enum):
    """
    The compression type for all data generated by the producer.
    """

    none = 'none'
    gzip = 'gzip'
    snappy = 'snappy'
    lz4 = 'lz4'
    zstd = 'zstd'


class SecurityProtocol(Enum):
    PLAINTEXT = 'PLAINTEXT'


@dataclass
class PLAINTEXT:
    """
    Protocol used to communicate with brokers.
    """

    security_protocol: SecurityProtocol


class SASLMechanism(Enum):
    """
    SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
    """

    PLAIN = 'PLAIN'


class SecurityProtocol1(Enum):
    SASL_PLAINTEXT = 'SASL_PLAINTEXT'


@dataclass
class SASLPLAINTEXT:
    """
    Protocol used to communicate with brokers.
    """

    sasl_jaas_config: str
    sasl_mechanism: SASLMechanism
    security_protocol: SecurityProtocol1


class SASLMechanism1(Enum):
    """
    SASL mechanism used for client connections. This may be any mechanism for which a security provider is available.
    """

    GSSAPI = 'GSSAPI'
    OAUTHBEARER = 'OAUTHBEARER'
    SCRAM_SHA_256 = 'SCRAM-SHA-256'
    SCRAM_SHA_512 = 'SCRAM-SHA-512'
    PLAIN = 'PLAIN'


class SecurityProtocol2(Enum):
    SASL_SSL = 'SASL_SSL'


@dataclass
class SASLSSL:
    """
    Protocol used to communicate with brokers.
    """

    sasl_jaas_config: str
    sasl_mechanism: SASLMechanism1
    security_protocol: SecurityProtocol2


@dataclass
class Kafka:
    acks: ACKs
    batch_size: int
    bootstrap_servers: str
    buffer_memory: str
    client_dns_lookup: ClientDNSLookup
    client_id: Optional[str]
    compression_type: CompressionType4
    delivery_timeout_ms: int
    destination: str
    enable_idempotence: bool
    linger_ms: str
    max_block_ms: str
    max_in_flight_requests_per_connection: int
    max_request_size: int
    protocol: Union[PLAINTEXT, SASLPLAINTEXT, SASLSSL]
    receive_buffer_bytes: int
    request_timeout_ms: int
    retries: int
    send_buffer_bytes: int
    socket_connection_setup_timeout_max_ms: str
    socket_connection_setup_timeout_ms: str
    test_topic: Optional[str]
    topic_pattern: str
    sync_producer: Optional[bool] = False


@dataclass
class Keen:
    api_key: str
    destination: str
    project_id: str
    infer_timestamp: Optional[bool] = True


@dataclass
class Kinesis:
    accessKey: str
    bufferSize: int
    destination: str
    endpoint: str
    privateKey: str
    region: str
    shardCount: int


class Mode12(Enum):
    openai = 'openai'


@dataclass
class OpenAI1:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    openai_key: str
    mode: Optional[Mode12] = Mode12.openai


class Mode13(Enum):
    fake = 'fake'


@dataclass
class Fake1:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Optional[Mode13] = Mode13.fake


class Mode14(Enum):
    pinecone = 'pinecone'


@dataclass
class Pinecone:
    """
    Pinecone is a popular vector store that can be used to store and retrieve embeddings. It is a managed service and can also be queried from outside of langchain.
    """

    index: str
    pinecone_environment: str
    pinecone_key: str
    mode: Optional[Mode14] = Mode14.pinecone


class Mode15(Enum):
    DocArrayHnswSearch = 'DocArrayHnswSearch'


@dataclass
class DocArrayHnswSearch:
    """
    DocArrayHnswSearch is a lightweight Document Index implementation provided by Docarray that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in hnswlib, and stores all other data in SQLite.
    """

    destination_path: str
    mode: Optional[Mode15] = Mode15.DocArrayHnswSearch


class Mode16(Enum):
    chroma_local = 'chroma_local'


@dataclass
class ChromaLocalPersistance:
    """
    Chroma is a popular vector store that can be used to store and retrieve embeddings. It will build its index in memory and persist it to disk by the end of the sync.
    """

    destination_path: str
    collection_name: Optional[str] = 'langchain'
    mode: Optional[Mode16] = Mode16.chroma_local


@dataclass
class ProcessingConfigModel1:
    chunk_size: int
    text_fields: List[str]
    chunk_overlap: Optional[int] = 0


@dataclass
class Langchain:
    destination: str
    embedding: Union[OpenAI1, Fake1]
    indexing: Union[Pinecone, DocArrayHnswSearch, ChromaLocalPersistance]
    processing: ProcessingConfigModel1


@dataclass
class Localjson:
    destination: str
    destination_path: str


@dataclass
class Mariadbcolumnstore:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str


@dataclass
class Meilisearch:
    api_key: Optional[str]
    destination: str
    host: str


class Mode17(Enum):
    openai = 'openai'


@dataclass
class OpenAI2:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    mode: Mode17
    openai_key: str


class Mode18(Enum):
    cohere = 'cohere'


@dataclass
class Cohere1:
    """
    Use the Cohere API to embed text.
    """

    cohere_key: str
    mode: Mode18


class Mode19(Enum):
    fake = 'fake'


@dataclass
class Fake2:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Mode19


class Mode20(Enum):
    azure_openai = 'azure_openai'


@dataclass
class AzureOpenAI1:
    """
    Use the Azure-hosted OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    api_base: str
    deployment: str
    mode: Mode20
    openai_key: str


class Mode21(Enum):
    openai_compatible = 'openai_compatible'


@dataclass
class OpenAICompatible1:
    """
    Use a service that's compatible with the OpenAI API to embed text.
    """

    base_url: str
    dimensions: int
    mode: Mode21
    api_key: Optional[str] = ''
    model_name: Optional[str] = 'text-embedding-ada-002'


class Mode22(Enum):
    token = 'token'


@dataclass
class APIToken:
    """
    Authenticate using an API token (suitable for Zilliz Cloud)
    """

    mode: Mode22
    token: str


class Mode23(Enum):
    username_password = 'username_password'


@dataclass
class UsernamePassword1:
    """
    Authenticate using username and password (suitable for self-managed Milvus clusters)
    """

    mode: Mode23
    password: str
    username: str


class Mode24(Enum):
    no_auth = 'no_auth'


@dataclass
class NoAuth:
    """
    Do not authenticate (suitable for locally running test clusters, do not use for clusters with public IP addresses)
    """

    mode: Mode24


@dataclass
class Indexing1:
    """
    Indexing configuration
    """

    auth: Union[APIToken, UsernamePassword1, NoAuth]
    collection: str
    host: str
    db: Optional[str] = ''
    text_field: Optional[str] = 'text'
    vector_field: Optional[str] = 'vector'


class Mode25(Enum):
    separator = 'separator'


@dataclass
class BySeparator1:
    """
    Split the text by the list of separators until the chunk size is reached, using the earlier mentioned separators where possible. This is useful for splitting text fields by paragraphs, sentences, words, etc.
    """

    mode: Mode25
    keep_separator: Optional[bool] = False
    separators: Optional[List[str]] = field(
        default_factory=lambda: ['"\\n\\n"', '"\\n"', '" "', '""']
    )


class Mode26(Enum):
    markdown = 'markdown'


@dataclass
class ByMarkdownHeader1:
    """
    Split the text by Markdown headers down to the specified header level. If the chunk size fits multiple sections, they will be combined into a single chunk.
    """

    mode: Mode26
    split_level: Optional[int] = 1


class Mode27(Enum):
    code = 'code'


@dataclass
class ByProgrammingLanguage1:
    """
    Split the text by suitable delimiters based on the programming language. This is useful for splitting code into chunks.
    """

    language: Language
    mode: Mode27


@dataclass
class ProcessingConfigModel2:
    chunk_size: int
    text_splitter: Optional[
        Union[BySeparator1, ByMarkdownHeader1, ByProgrammingLanguage1]
    ]
    chunk_overlap: Optional[int] = 0
    field_name_mappings: Optional[List[FieldNameMappingConfigModel]] = field(
        default_factory=lambda: []
    )
    metadata_fields: Optional[List[str]] = field(default_factory=lambda: [])
    text_fields: Optional[List[str]] = field(default_factory=lambda: [])


@dataclass
class Milvus:
    """
    The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
    as well as to provide type safety for the configuration passed to the destination.

    The configuration model is composed of four parts:
    * Processing configuration
    * Embedding configuration
    * Indexing configuration
    * Advanced configuration

    Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
    """

    destination: str
    embedding: Union[OpenAI2, Cohere1, Fake2, AzureOpenAI1, OpenAICompatible1]
    indexing: Indexing1
    processing: ProcessingConfigModel2
    omit_raw_text: Optional[bool] = False


@dataclass
class None_1:
    """
    None.
    """

    authorization: str


@dataclass
class LoginPassword:
    """
    Login/Password.
    """

    authorization: str
    password: str
    username: str


class Instance(Enum):
    standalone = 'standalone'


@dataclass
class StandaloneMongoDbInstance:
    """
    MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.
    """

    host: str
    instance: Instance
    port: int
    tls: Optional[bool] = False


class Instance1(Enum):
    replica = 'replica'


@dataclass
class ReplicaSet:
    """
    MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.
    """

    instance: Instance1
    replica_set: Optional[str]
    server_addresses: str


class Instance2(Enum):
    atlas = 'atlas'


@dataclass
class MongoDBAtlas:
    """
    MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.
    """

    cluster_url: str
    instance: Instance2


@dataclass
class Mongodb:
    auth_type: Union[None_1, LoginPassword]
    database: str
    destination: str
    instance_type: Optional[Union[StandaloneMongoDbInstance, ReplicaSet, MongoDBAtlas]]
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]


class MessageQoS(Enum):
    """
    Quality of service used for each message to be delivered.
    """

    AT_MOST_ONCE = 'AT_MOST_ONCE'
    AT_LEAST_ONCE = 'AT_LEAST_ONCE'
    EXACTLY_ONCE = 'EXACTLY_ONCE'


@dataclass
class Mqtt:
    automatic_reconnect: bool
    broker_host: str
    broker_port: int
    clean_session: bool
    client: Optional[str]
    connect_timeout: int
    destination: str
    message_qos: MessageQoS
    message_retained: bool
    password: Optional[str]
    publisher_sync: bool
    topic_pattern: str
    topic_test: Optional[str]
    use_tls: bool
    username: Optional[str]


class SslMethod(Enum):
    unencrypted = 'unencrypted'


@dataclass
class Unencrypted:
    """
    The data transfer will not be encrypted.
    """

    ssl_method: SslMethod


class SslMethod1(Enum):
    encrypted_trust_server_certificate = 'encrypted_trust_server_certificate'


@dataclass
class EncryptedTrustServerCertificate:
    """
    Use the certificate provided by the server without verification. (For testing purposes only!)
    """

    ssl_method: SslMethod1


class SslMethod2(Enum):
    encrypted_verify_certificate = 'encrypted_verify_certificate'


@dataclass
class EncryptedVerifyCertificate:
    """
    Verify and use the certificate provided by the server.
    """

    hostNameInCertificate: Optional[str]
    ssl_method: SslMethod2


@dataclass
class Mssql:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    schema_: str
    ssl_method: Optional[
        Union[Unencrypted, EncryptedTrustServerCertificate, EncryptedVerifyCertificate]
    ]
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str


@dataclass
class Mysql:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    ssl: Optional[bool] = True


class EncryptionMethod(Enum):
    unencrypted = 'unencrypted'


@dataclass
class Unencrypted1:
    """
    Data transfer will not be encrypted.
    """

    encryption_method: EncryptionMethod


class EncryptionAlgorithm(Enum):
    """
    This parameter defines the database encryption algorithm.
    """

    AES256 = 'AES256'
    RC4_56 = 'RC4_56'
    field_3DES168 = '3DES168'


class EncryptionMethod1(Enum):
    client_nne = 'client_nne'


@dataclass
class NativeNetworkEncryptionNNE:
    """
    The native network encryption gives you the ability to encrypt database connections, without the configuration overhead of TCP/IP and SSL/TLS and without the need to open and listen on different ports.
    """

    encryption_method: EncryptionMethod1
    encryption_algorithm: Optional[EncryptionAlgorithm] = EncryptionAlgorithm.AES256


class EncryptionMethod2(Enum):
    encrypted_verify_certificate = 'encrypted_verify_certificate'


@dataclass
class TLSEncryptedVerifyCertificate:
    """
    Verify and use the certificate provided by the server.
    """

    encryption_method: EncryptionMethod2
    ssl_certificate: str


@dataclass
class Oracle:
    destination: str
    encryption: Optional[
        Union[Unencrypted1, NativeNetworkEncryptionNNE, TLSEncryptedVerifyCertificate]
    ]
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    sid: str
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    schema_: Optional[str] = 'airbyte'


class Mode28(Enum):
    openai = 'openai'


@dataclass
class OpenAI3:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    mode: Mode28
    openai_key: str


class Mode29(Enum):
    cohere = 'cohere'


@dataclass
class Cohere2:
    """
    Use the Cohere API to embed text.
    """

    cohere_key: str
    mode: Mode29


class Mode30(Enum):
    fake = 'fake'


@dataclass
class Fake3:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Mode30


class Mode31(Enum):
    azure_openai = 'azure_openai'


@dataclass
class AzureOpenAI2:
    """
    Use the Azure-hosted OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    api_base: str
    deployment: str
    mode: Mode31
    openai_key: str


class Mode32(Enum):
    openai_compatible = 'openai_compatible'


@dataclass
class OpenAICompatible2:
    """
    Use a service that's compatible with the OpenAI API to embed text.
    """

    base_url: str
    dimensions: int
    mode: Mode32
    api_key: Optional[str] = ''
    model_name: Optional[str] = 'text-embedding-ada-002'


@dataclass
class Indexing2:
    """
    Pinecone is a popular vector store that can be used to store and retrieve embeddings.
    """

    index: str
    pinecone_environment: str
    pinecone_key: str


class Mode33(Enum):
    separator = 'separator'


@dataclass
class BySeparator2:
    """
    Split the text by the list of separators until the chunk size is reached, using the earlier mentioned separators where possible. This is useful for splitting text fields by paragraphs, sentences, words, etc.
    """

    mode: Mode33
    keep_separator: Optional[bool] = False
    separators: Optional[List[str]] = field(
        default_factory=lambda: ['"\\n\\n"', '"\\n"', '" "', '""']
    )


class Mode34(Enum):
    markdown = 'markdown'


@dataclass
class ByMarkdownHeader2:
    """
    Split the text by Markdown headers down to the specified header level. If the chunk size fits multiple sections, they will be combined into a single chunk.
    """

    mode: Mode34
    split_level: Optional[int] = 1


class Mode35(Enum):
    code = 'code'


@dataclass
class ByProgrammingLanguage2:
    """
    Split the text by suitable delimiters based on the programming language. This is useful for splitting code into chunks.
    """

    language: Language
    mode: Mode35


@dataclass
class ProcessingConfigModel3:
    chunk_size: int
    text_splitter: Optional[
        Union[BySeparator2, ByMarkdownHeader2, ByProgrammingLanguage2]
    ]
    chunk_overlap: Optional[int] = 0
    field_name_mappings: Optional[List[FieldNameMappingConfigModel]] = field(
        default_factory=lambda: []
    )
    metadata_fields: Optional[List[str]] = field(default_factory=lambda: [])
    text_fields: Optional[List[str]] = field(default_factory=lambda: [])


@dataclass
class Pinecone1:
    """
    The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
    as well as to provide type safety for the configuration passed to the destination.

    The configuration model is composed of four parts:
    * Processing configuration
    * Embedding configuration
    * Indexing configuration
    * Advanced configuration

    Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
    """

    destination: str
    embedding: Union[OpenAI3, Cohere2, Fake3, AzureOpenAI2, OpenAICompatible2]
    indexing: Indexing2
    processing: ProcessingConfigModel3
    omit_raw_text: Optional[bool] = False


class Mode36(Enum):
    disable = 'disable'


@dataclass
class Disable:
    """
    Disable SSL.
    """

    mode: Mode36


class Mode37(Enum):
    allow = 'allow'


@dataclass
class Allow:
    """
    Allow SSL mode.
    """

    mode: Mode37


class Mode38(Enum):
    prefer = 'prefer'


@dataclass
class Prefer:
    """
    Prefer SSL mode.
    """

    mode: Mode38


class Mode39(Enum):
    require = 'require'


@dataclass
class Require:
    """
    Require SSL mode.
    """

    mode: Mode39


class Mode40(Enum):
    verify_ca = 'verify-ca'


@dataclass
class VerifyCa:
    """
    Verify-ca SSL mode.
    """

    ca_certificate: str
    client_key_password: Optional[str]
    mode: Mode40


class Mode41(Enum):
    verify_full = 'verify-full'


@dataclass
class VerifyFull:
    """
    Verify-full SSL mode.
    """

    ca_certificate: str
    client_certificate: str
    client_key: str
    client_key_password: Optional[str]
    mode: Mode41


@dataclass
class Postgres:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    schema_: str
    ssl_mode: Optional[Union[Disable, Allow, Prefer, Require, VerifyCa, VerifyFull]]
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    ssl: Optional[bool] = False


@dataclass
class Pubsub:
    batching_enabled: bool
    credentials_json: str
    destination: str
    ordering_enabled: bool
    project_id: str
    topic_id: str
    batching_delay_threshold: Optional[int] = 1
    batching_element_count_threshold: Optional[int] = 1
    batching_request_bytes_threshold: Optional[int] = 1


class CompressionType5(Enum):
    """
    Compression type for the producer.
    """

    NONE = 'NONE'
    LZ4 = 'LZ4'
    ZLIB = 'ZLIB'
    ZSTD = 'ZSTD'
    SNAPPY = 'SNAPPY'


class TopicType(Enum):
    """
    It identifies type of topic. Pulsar supports two kind of topics: persistent and non-persistent. In persistent topic, all messages are durably persisted on disk (that means on multiple disks unless the broker is standalone), whereas non-persistent topic does not persist message into storage disk.
    """

    persistent = 'persistent'
    non_persistent = 'non-persistent'


@dataclass
class Pulsar:
    batching_enabled: bool
    batching_max_messages: int
    batching_max_publish_delay: int
    block_if_queue_full: bool
    brokers: str
    compression_type: CompressionType5
    destination: str
    max_pending_messages: int
    max_pending_messages_across_partitions: int
    producer_name: Optional[str]
    send_timeout_ms: int
    topic_namespace: str
    topic_pattern: str
    topic_tenant: str
    topic_test: Optional[str]
    topic_type: TopicType
    use_tls: bool
    producer_sync: Optional[bool] = False


class Mode42(Enum):
    openai = 'openai'


@dataclass
class OpenAI4:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    mode: Mode42
    openai_key: str


class Mode43(Enum):
    cohere = 'cohere'


@dataclass
class Cohere3:
    """
    Use the Cohere API to embed text.
    """

    cohere_key: str
    mode: Mode43


class Mode44(Enum):
    fake = 'fake'


@dataclass
class Fake4:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Mode44


class Mode45(Enum):
    azure_openai = 'azure_openai'


@dataclass
class AzureOpenAI3:
    """
    Use the Azure-hosted OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    api_base: str
    deployment: str
    mode: Mode45
    openai_key: str


class Mode46(Enum):
    openai_compatible = 'openai_compatible'


@dataclass
class OpenAICompatible3:
    """
    Use a service that's compatible with the OpenAI API to embed text.
    """

    base_url: str
    dimensions: int
    mode: Mode46
    api_key: Optional[str] = ''
    model_name: Optional[str] = 'text-embedding-ada-002'


class Mode47(Enum):
    api_key_auth = 'api_key_auth'


@dataclass
class ApiKeyAuth:
    """
    Method to authenticate with the Qdrant Instance
    """

    api_key: str
    mode: Optional[Mode47] = Mode47.api_key_auth


class Mode48(Enum):
    no_auth = 'no_auth'


@dataclass
class NoAuth1:
    """
    Method to authenticate with the Qdrant Instance
    """

    mode: Optional[Mode48] = Mode48.no_auth


class DistanceMetric(Enum):
    """
    The Distance metric used to measure similarities among vectors. This field is only used if the collection defined in the does not exist yet and is created automatically by the connector.
    """

    dot = 'dot'
    cos = 'cos'
    euc = 'euc'


@dataclass
class Indexing3:
    """
    Indexing configuration
    """

    collection: str
    url: str
    auth_method: Optional[Union[ApiKeyAuth, NoAuth1]] = 'api_key_auth'
    distance_metric: Optional[DistanceMetric] = DistanceMetric.cos
    prefer_grpc: Optional[bool] = True
    text_field: Optional[str] = 'text'


class Mode49(Enum):
    separator = 'separator'


@dataclass
class BySeparator3:
    """
    Split the text by the list of separators until the chunk size is reached, using the earlier mentioned separators where possible. This is useful for splitting text fields by paragraphs, sentences, words, etc.
    """

    mode: Mode49
    keep_separator: Optional[bool] = False
    separators: Optional[List[str]] = field(
        default_factory=lambda: ['"\\n\\n"', '"\\n"', '" "', '""']
    )


class Mode50(Enum):
    markdown = 'markdown'


@dataclass
class ByMarkdownHeader3:
    """
    Split the text by Markdown headers down to the specified header level. If the chunk size fits multiple sections, they will be combined into a single chunk.
    """

    mode: Mode50
    split_level: Optional[int] = 1


class Mode51(Enum):
    code = 'code'


@dataclass
class ByProgrammingLanguage3:
    """
    Split the text by suitable delimiters based on the programming language. This is useful for splitting code into chunks.
    """

    language: Language
    mode: Mode51


@dataclass
class ProcessingConfigModel4:
    chunk_size: int
    text_splitter: Optional[
        Union[BySeparator3, ByMarkdownHeader3, ByProgrammingLanguage3]
    ]
    chunk_overlap: Optional[int] = 0
    field_name_mappings: Optional[List[FieldNameMappingConfigModel]] = field(
        default_factory=lambda: []
    )
    metadata_fields: Optional[List[str]] = field(default_factory=lambda: [])
    text_fields: Optional[List[str]] = field(default_factory=lambda: [])


@dataclass
class Qdrant:
    """
    The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
    as well as to provide type safety for the configuration passed to the destination.

    The configuration model is composed of four parts:
    * Processing configuration
    * Embedding configuration
    * Indexing configuration
    * Advanced configuration

    Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
    """

    destination: str
    embedding: Union[OpenAI4, Cohere3, Fake4, AzureOpenAI3, OpenAICompatible3]
    indexing: Indexing3
    processing: ProcessingConfigModel4
    omit_raw_text: Optional[bool] = False


class Codec6(Enum):
    no_compression = 'no compression'


@dataclass
class NoCompression3:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec6


class Codec7(Enum):
    Deflate = 'Deflate'


@dataclass
class Deflate1:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec7
    compression_level: int


class Codec8(Enum):
    bzip2 = 'bzip2'


@dataclass
class Bzip21:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec8


class Codec9(Enum):
    xz = 'xz'


@dataclass
class Xz1:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec9
    compression_level: int


class Codec10(Enum):
    zstandard = 'zstandard'


@dataclass
class Zstandard1:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec10
    compression_level: int
    include_checksum: Optional[bool] = False


class Codec11(Enum):
    snappy = 'snappy'


@dataclass
class Snappy1:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec11


class FormatType6(Enum):
    Avro = 'Avro'


@dataclass
class AvroApacheAvro1:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression_codec: Union[NoCompression3, Deflate1, Bzip21, Xz1, Zstandard1, Snappy1]
    format_type: FormatType6


class CompressionType6(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression4:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType6] = CompressionType6.No_Compression


class CompressionType7(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP2:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType7] = CompressionType7.GZIP


class FormatType7(Enum):
    CSV = 'CSV'


@dataclass
class CSVCommaSeparatedValues2:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression: Optional[Union[NoCompression4, GZIP2]]
    flattening: NormalizationFlattening
    format_type: FormatType7


class CompressionType8(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression5:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType8] = CompressionType8.No_Compression


class CompressionType9(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP3:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType9] = CompressionType9.GZIP


class FormatType8(Enum):
    JSONL = 'JSONL'


@dataclass
class JSONLinesNewlineDelimitedJSON3:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression: Optional[Union[NoCompression5, GZIP3]]
    format_type: FormatType8


@dataclass
class R2:
    access_key_id: str
    account_id: str
    destination: str
    file_name_pattern: Optional[str]
    format: Union[
        AvroApacheAvro1, CSVCommaSeparatedValues2, JSONLinesNewlineDelimitedJSON3
    ]
    s3_bucket_name: str
    s3_bucket_path: str
    s3_path_format: Optional[str]
    secret_access_key: str


@dataclass
class Rabbitmq:
    destination: str
    exchange: Optional[str]
    host: str
    password: Optional[str]
    port: Optional[int]
    routing_key: str
    username: Optional[str]
    virtual_host: Optional[str]
    ssl: Optional[bool] = True


class CacheType(Enum):
    """
    Redis cache type to store data in.
    """

    hash = 'hash'


class Mode52(Enum):
    disable = 'disable'


@dataclass
class Disable1:
    """
    Disable SSL.
    """

    mode: Mode52


class Mode53(Enum):
    verify_full = 'verify-full'


@dataclass
class VerifyFull1:
    """
    Verify-full SSL mode.
    """

    ca_certificate: str
    client_certificate: str
    client_key: str
    client_key_password: Optional[str]
    mode: Mode53


@dataclass
class Redis:
    cache_type: CacheType
    destination: str
    host: str
    password: Optional[str]
    port: int
    ssl_mode: Optional[Union[Disable1, VerifyFull1]]
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    ssl: Optional[bool] = False


class CompressionType10(Enum):
    """
    The compression type for all data generated by the producer.
    """

    none = 'none'
    gzip = 'gzip'
    snappy = 'snappy'
    lz4 = 'lz4'
    zstd = 'zstd'


@dataclass
class Redpanda:
    batch_size: int
    bootstrap_servers: str
    buffer_memory: str
    compression_type: CompressionType10
    destination: str
    retries: int
    socket_connection_setup_timeout_max_ms: Optional[int]
    socket_connection_setup_timeout_ms: Optional[int]
    topic_num_partitions: Optional[int]
    topic_replication_factor: Optional[int]


class EncryptionType(Enum):
    none = 'none'


@dataclass
class NoEncryption:
    """
    Staging data will be stored in plaintext.
    """

    encryption_type: EncryptionType


class EncryptionType1(Enum):
    aes_cbc_envelope = 'aes_cbc_envelope'


@dataclass
class AESCBCEnvelopeEncryption:
    """
    Staging data will be encrypted using AES-CBC envelope encryption.
    """

    encryption_type: EncryptionType1
    key_encrypting_key: Optional[str]


class S3BucketRegion3(Enum):
    """
    The region of the S3 staging bucket.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'


@dataclass
class AWSS3Staging:
    """
    <i>(recommended)</i> Uploads data to S3 and then uses a COPY to insert the data into Redshift. COPY is recommended for production workloads for better speed and scalability. See <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html">AWS docs</a> for more details.
    """

    access_key_id: str
    file_name_pattern: Optional[str]
    method: str
    s3_bucket_name: str
    s3_bucket_path: Optional[str]
    s3_bucket_region: S3BucketRegion3
    secret_access_key: str
    encryption: Optional[Union[NoEncryption, AESCBCEnvelopeEncryption]] = field(
        default_factory=lambda: {'encryption_type': 'none'}
    )
    file_buffer_count: Optional[int] = 10
    purge_staging_data: Optional[bool] = True


@dataclass
class Standard:
    """
    <i>(not recommended)</i> Direct loading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In all other cases, you should use S3 uploading.
    """

    method: str


@dataclass
class Redshift:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: str
    port: int
    raw_data_schema: Optional[str]
    schema_: str
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    uploading_method: Optional[Union[AWSS3Staging, Standard]]
    use_1s1t_format: Optional[bool]
    username: str


@dataclass
class Rockset:
    api_key: str
    destination: str
    workspace: str
    api_server: Optional[str] = 'https://api.rs2.usw2.rockset.com'


class CompressionType11(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression6:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType11] = CompressionType11.No_Compression


class CompressionType12(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP4:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType12] = CompressionType12.GZIP


class Flattening(Enum):
    """
    Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
    """

    No_flattening = 'No flattening'
    Root_level_flattening = 'Root level flattening'


@dataclass
class JSONLinesNewlineDelimitedJSON4:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression: Optional[Union[NoCompression6, GZIP4]]
    format_type: FormatType8
    flattening: Optional[Flattening] = Flattening.Root_level_flattening


class SerializationLibrary(Enum):
    """
    The library that your query engine will use for reading and writing data in your lake.
    """

    org_openx_data_jsonserde_JsonSerDe = 'org.openx.data.jsonserde.JsonSerDe'
    org_apache_hive_hcatalog_data_JsonSerDe = 'org.apache.hive.hcatalog.data.JsonSerDe'


class S3BucketRegion4(Enum):
    """
    The region of the S3 bucket. See <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">here</a> for all region codes.
    """

    field_ = ''
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'
    af_south_1 = 'af-south-1'
    ap_east_1 = 'ap-east-1'
    ap_south_1 = 'ap-south-1'
    ap_northeast_1 = 'ap-northeast-1'
    ap_northeast_2 = 'ap-northeast-2'
    ap_northeast_3 = 'ap-northeast-3'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    cn_north_1 = 'cn-north-1'
    cn_northwest_1 = 'cn-northwest-1'
    eu_central_1 = 'eu-central-1'
    eu_north_1 = 'eu-north-1'
    eu_south_1 = 'eu-south-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    sa_east_1 = 'sa-east-1'
    me_south_1 = 'me-south-1'
    us_gov_east_1 = 'us-gov-east-1'
    us_gov_west_1 = 'us-gov-west-1'


@dataclass
class S3glue:
    access_key_id: Optional[str]
    destination: str
    file_name_pattern: Optional[str]
    format: JSONLinesNewlineDelimitedJSON4
    glue_database: str
    glue_serialization_library: SerializationLibrary
    s3_bucket_name: str
    s3_bucket_path: str
    s3_bucket_region: S3BucketRegion4
    s3_path_format: Optional[str]
    secret_access_key: Optional[str]
    s3_endpoint: Optional[str] = ''


class Codec12(Enum):
    no_compression = 'no compression'


@dataclass
class NoCompression7:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec12


class Codec13(Enum):
    Deflate = 'Deflate'


@dataclass
class Deflate2:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec13
    compression_level: int


class Codec14(Enum):
    bzip2 = 'bzip2'


@dataclass
class Bzip22:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec14


class Codec15(Enum):
    xz = 'xz'


@dataclass
class Xz2:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec15
    compression_level: int


class Codec16(Enum):
    zstandard = 'zstandard'


@dataclass
class Zstandard2:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec16
    compression_level: int
    include_checksum: Optional[bool] = False


class Codec17(Enum):
    snappy = 'snappy'


@dataclass
class Snappy2:
    """
    The compression algorithm used to compress data. Default to no compression.
    """

    codec: Codec17


class FormatType10(Enum):
    Avro = 'Avro'


@dataclass
class AvroApacheAvro2:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression_codec: Union[NoCompression7, Deflate2, Bzip22, Xz2, Zstandard2, Snappy2]
    format_type: FormatType10


class CompressionType13(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression8:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType13] = CompressionType13.No_Compression


class CompressionType14(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP5:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".csv.gz").
    """

    compression_type: Optional[CompressionType14] = CompressionType14.GZIP


class Flattening1(Enum):
    """
    Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.
    """

    No_flattening = 'No flattening'
    Root_level_flattening = 'Root level flattening'


class FormatType11(Enum):
    CSV = 'CSV'


@dataclass
class CSVCommaSeparatedValues3:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression: Optional[Union[NoCompression8, GZIP5]]
    flattening: Flattening1
    format_type: FormatType11


class CompressionType15(Enum):
    No_Compression = 'No Compression'


@dataclass
class NoCompression9:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType15] = CompressionType15.No_Compression


class CompressionType16(Enum):
    GZIP = 'GZIP'


@dataclass
class GZIP6:
    """
    Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: ".jsonl.gz").
    """

    compression_type: Optional[CompressionType16] = CompressionType16.GZIP


class Flattening2(Enum):
    """
    Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.
    """

    No_flattening = 'No flattening'
    Root_level_flattening = 'Root level flattening'


class FormatType12(Enum):
    JSONL = 'JSONL'


@dataclass
class JSONLinesNewlineDelimitedJSON5:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    compression: Optional[Union[NoCompression9, GZIP6]]
    format_type: FormatType12
    flattening: Optional[Flattening2] = Flattening2.No_flattening


class FormatType13(Enum):
    Parquet = 'Parquet'


@dataclass
class ParquetColumnarStorage2:
    """
    Format of the data output. See <a href="https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema">here</a> for more details
    """

    format_type: FormatType13
    block_size_mb: Optional[int] = 128
    compression_codec: Optional[CompressionCodec] = CompressionCodec.UNCOMPRESSED
    dictionary_encoding: Optional[bool] = True
    dictionary_page_size_kb: Optional[int] = 1024
    max_padding_size_mb: Optional[int] = 8
    page_size_kb: Optional[int] = 1024


@dataclass
class S31:
    access_key_id: Optional[str]
    destination: str
    file_name_pattern: Optional[str]
    format: Union[
        AvroApacheAvro2,
        CSVCommaSeparatedValues3,
        JSONLinesNewlineDelimitedJSON5,
        ParquetColumnarStorage2,
    ]
    s3_bucket_name: str
    s3_bucket_path: str
    s3_bucket_region: S3BucketRegion4
    s3_path_format: Optional[str]
    secret_access_key: Optional[str]
    s3_endpoint: Optional[str] = ''


@dataclass
class Scylla:
    address: str
    destination: str
    keyspace: str
    password: str
    port: int
    username: str
    replication: Optional[int] = 1


@dataclass
class Selectdb:
    cluster_name: str
    database: str
    destination: str
    jdbc_url: str
    load_url: str
    password: str
    user_name: str


@dataclass
class Sftpjson:
    destination: str
    destination_path: str
    host: str
    password: str
    username: str
    port: Optional[int] = 22


class AuthType(Enum):
    Key_Pair_Authentication = 'Key Pair Authentication'


@dataclass
class KeyPairAuthentication:
    private_key: str
    private_key_password: Optional[str]
    auth_type: Optional[AuthType] = AuthType.Key_Pair_Authentication


class AuthType1(Enum):
    Username_and_Password = 'Username and Password'


@dataclass
class UsernameAndPassword:
    password: str
    auth_type: Optional[AuthType1] = AuthType1.Username_and_Password


@dataclass
class Snowflake:
    credentials: Optional[
        Union[OAuth2.Field0, KeyPairAuthentication, UsernameAndPassword]
    ]
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    raw_data_schema: Optional[str]
    role: str
    schema_: str
    username: str
    warehouse: str
    disable_type_dedupe: Optional[bool] = False


@dataclass
class Sqlite:
    destination: str
    destination_path: str


class ObjectStoreType(Enum):
    S3 = 'S3'


class S3BucketRegion6(Enum):
    """
    The region of the S3 bucket.
    """

    ap_northeast_1 = 'ap-northeast-1'
    ap_southeast_1 = 'ap-southeast-1'
    ap_southeast_2 = 'ap-southeast-2'
    ca_central_1 = 'ca-central-1'
    eu_central_1 = 'eu-central-1'
    eu_west_1 = 'eu-west-1'
    eu_west_2 = 'eu-west-2'
    eu_west_3 = 'eu-west-3'
    us_east_1 = 'us-east-1'
    us_east_2 = 'us-east-2'
    us_west_1 = 'us-west-1'
    us_west_2 = 'us-west-2'


@dataclass
class AmazonS31:
    """
    Temporary storage on which temporary Iceberg table is created.
    """

    object_store_type: ObjectStoreType
    s3_access_key_id: str
    s3_bucket_name: str
    s3_bucket_path: str
    s3_bucket_region: S3BucketRegion6
    s3_secret_access_key: str


@dataclass
class Starburstgalaxy:
    accept_terms: bool
    catalog: str
    destination: str
    password: str
    server_hostname: str
    staging_object_store: AmazonS31
    username: str
    catalog_schema: Optional[str] = 'public'
    port: Optional[str] = '443'
    purge_staging_table: Optional[bool] = True


class Mode54(Enum):
    disable = 'disable'


@dataclass
class Disable2:
    """
    Disable SSL.
    """

    mode: Mode54


class Mode55(Enum):
    allow = 'allow'


@dataclass
class Allow1:
    """
    Allow SSL mode.
    """

    mode: Mode55


class Mode56(Enum):
    prefer = 'prefer'


@dataclass
class Prefer1:
    """
    Prefer SSL mode.
    """

    mode: Mode56


class Mode57(Enum):
    require = 'require'


@dataclass
class Require1:
    """
    Require SSL mode.
    """

    mode: Mode57


class Mode58(Enum):
    verify_ca = 'verify-ca'


@dataclass
class VerifyCa1:
    """
    Verify-ca SSL mode.
    """

    mode: Mode58
    ssl_ca_certificate: str


class Mode59(Enum):
    verify_full = 'verify-full'


@dataclass
class VerifyFull2:
    """
    Verify-full SSL mode.
    """

    mode: Mode59
    ssl_ca_certificate: str


@dataclass
class Teradata:
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    ssl_mode: Optional[
        Union[Disable2, Allow1, Prefer1, Require1, VerifyCa1, VerifyFull2]
    ]
    username: str
    schema_: Optional[str] = 'airbyte_td'
    ssl: Optional[bool] = False


@dataclass
class Tidb:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    port: int
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str
    password: Optional[str] = ''
    ssl: Optional[bool] = False


@dataclass
class Timeplus:
    apikey: str
    destination: str
    endpoint: str


@dataclass
class Typesense:
    api_key: str
    batch_size: Optional[int]
    destination: str
    host: str
    port: Optional[str]
    protocol: Optional[str]


@dataclass
class Vertica:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    schema_: str
    tunnel_method: Optional[
        Union[NoTunnel, SSHKeyAuthentication, PasswordAuthentication]
    ]
    username: str


class Mode60(Enum):
    no_embedding = 'no_embedding'


@dataclass
class NoExternalEmbedding:
    """
    Do not calculate and pass embeddings to Weaviate. Suitable for clusters with configured vectorizers to calculate embeddings within Weaviate or for classes that should only support regular text search.
    """

    mode: Mode60


class Mode61(Enum):
    azure_openai = 'azure_openai'


@dataclass
class AzureOpenAI4:
    """
    Use the Azure-hosted OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    api_base: str
    deployment: str
    mode: Mode61
    openai_key: str


class Mode62(Enum):
    openai = 'openai'


@dataclass
class OpenAI5:
    """
    Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions.
    """

    mode: Mode62
    openai_key: str


class Mode63(Enum):
    cohere = 'cohere'


@dataclass
class Cohere4:
    """
    Use the Cohere API to embed text.
    """

    cohere_key: str
    mode: Mode63


class Mode64(Enum):
    from_field = 'from_field'


@dataclass
class FromField1:
    """
    Use a field in the record as the embedding. This is useful if you already have an embedding for your data and want to store it in the vector store.
    """

    dimensions: int
    field_name: str
    mode: Mode64


class Mode65(Enum):
    fake = 'fake'


@dataclass
class Fake5:
    """
    Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs.
    """

    mode: Mode65


class Mode66(Enum):
    openai_compatible = 'openai_compatible'


@dataclass
class OpenAICompatible4:
    """
    Use a service that's compatible with the OpenAI API to embed text.
    """

    base_url: str
    dimensions: int
    mode: Mode66
    api_key: Optional[str] = ''
    model_name: Optional[str] = 'text-embedding-ada-002'


@dataclass
class Header:
    header_key: str
    value: str


class Mode67(Enum):
    token = 'token'


@dataclass
class APIToken1:
    """
    Authenticate using an API token (suitable for Weaviate Cloud)
    """

    mode: Mode67
    token: str


class Mode68(Enum):
    username_password = 'username_password'


@dataclass
class UsernamePassword2:
    """
    Authenticate using username and password (suitable for self-managed Weaviate clusters)
    """

    mode: Mode68
    password: str
    username: str


class Mode69(Enum):
    no_auth = 'no_auth'


@dataclass
class NoAuthentication:
    """
    Do not authenticate (suitable for locally running test clusters, do not use for clusters with public IP addresses)
    """

    mode: Mode69


class DefaultVectorizer(Enum):
    """
    The vectorizer to use if new classes need to be created
    """

    none = 'none'
    text2vec_cohere = 'text2vec-cohere'
    text2vec_huggingface = 'text2vec-huggingface'
    text2vec_openai = 'text2vec-openai'
    text2vec_palm = 'text2vec-palm'
    text2vec_contextionary = 'text2vec-contextionary'
    text2vec_transformers = 'text2vec-transformers'
    text2vec_gpt4all = 'text2vec-gpt4all'


@dataclass
class Indexing4:
    """
    Indexing configuration
    """

    auth: Union[APIToken1, UsernamePassword2, NoAuthentication]
    host: str
    additional_headers: Optional[List[Header]] = field(default_factory=lambda: [])
    batch_size: Optional[int] = 128
    default_vectorizer: Optional[DefaultVectorizer] = DefaultVectorizer.none
    text_field: Optional[str] = 'text'


class Mode70(Enum):
    separator = 'separator'


@dataclass
class BySeparator4:
    """
    Split the text by the list of separators until the chunk size is reached, using the earlier mentioned separators where possible. This is useful for splitting text fields by paragraphs, sentences, words, etc.
    """

    mode: Mode70
    keep_separator: Optional[bool] = False
    separators: Optional[List[str]] = field(
        default_factory=lambda: ['"\\n\\n"', '"\\n"', '" "', '""']
    )


class Mode71(Enum):
    markdown = 'markdown'


@dataclass
class ByMarkdownHeader4:
    """
    Split the text by Markdown headers down to the specified header level. If the chunk size fits multiple sections, they will be combined into a single chunk.
    """

    mode: Mode71
    split_level: Optional[int] = 1


class Mode72(Enum):
    code = 'code'


@dataclass
class ByProgrammingLanguage4:
    """
    Split the text by suitable delimiters based on the programming language. This is useful for splitting code into chunks.
    """

    language: Language
    mode: Mode72


@dataclass
class ProcessingConfigModel5:
    chunk_size: int
    text_splitter: Optional[
        Union[BySeparator4, ByMarkdownHeader4, ByProgrammingLanguage4]
    ]
    chunk_overlap: Optional[int] = 0
    field_name_mappings: Optional[List[FieldNameMappingConfigModel]] = field(
        default_factory=lambda: []
    )
    metadata_fields: Optional[List[str]] = field(default_factory=lambda: [])
    text_fields: Optional[List[str]] = field(default_factory=lambda: [])


@dataclass
class Weaviate:
    """
    The configuration model for the Vector DB based destinations. This model is used to generate the UI for the destination configuration,
    as well as to provide type safety for the configuration passed to the destination.

    The configuration model is composed of four parts:
    * Processing configuration
    * Embedding configuration
    * Indexing configuration
    * Advanced configuration

    Processing, embedding and advanced configuration are provided by this base class, while the indexing configuration is provided by the destination connector in the sub class.
    """

    destination: str
    embedding: Union[
        NoExternalEmbedding,
        AzureOpenAI4,
        OpenAI5,
        Cohere4,
        FromField1,
        Fake5,
        OpenAICompatible4,
    ]
    indexing: Indexing4
    processing: ProcessingConfigModel5
    omit_raw_text: Optional[bool] = False


@dataclass
class Xata:
    api_key: str
    db_url: str
    destination: str


@dataclass
class Yugabytedb:
    database: str
    destination: str
    host: str
    jdbc_url_params: Optional[str]
    password: Optional[str]
    port: int
    schema_: str
    username: str


@dataclass
class Airbytedevmatecloud:
    destination: str
    privateKey: str
    streamId: str


Destination = Union[
    Amazonsqs,
    Awsdatalake,
    Azureblobstorage,
    Bigquery,
    Cassandra,
    Chroma,
    Clickhouse,
    Convex,
    Csv,
    Cumulio,
    Databend,
    Databricks,
    Doris,
    Duckdb,
    Dynamodb,
    E2etest,
    Elasticsearch,
    Exasol,
    Firebolt,
    Firestore,
    Gcs,
    Googlesheets,
    Iceberg,
    Kafka,
    Keen,
    Kinesis,
    Langchain,
    Localjson,
    Mariadbcolumnstore,
    Meilisearch,
    Milvus,
    Mongodb,
    Mqtt,
    Mssql,
    Mysql,
    Oracle,
    Pinecone1,
    Postgres,
    Pubsub,
    Pulsar,
    Qdrant,
    R2,
    Rabbitmq,
    Redis,
    Redpanda,
    Redshift,
    Rockset,
    S3glue,
    S31,
    Scylla,
    Selectdb,
    Sftpjson,
    Snowflake,
    Sqlite,
    Starburstgalaxy,
    Teradata,
    Tidb,
    Timeplus,
    Typesense,
    Vertica,
    Weaviate,
    Xata,
    Yugabytedb,
    Airbytedevmatecloud,
]
