{"instillUIOrder":0,"properties":{"chat_history":{"description":"Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should adhere to the format: : {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.","instillAcceptFormats":["structured/chat_messages"],"instillShortDescription":"Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should adhere to the format: : {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.","instillUIOrder":4,"instillUpstreamTypes":["value","reference"],"items":{"properties":{"content":{"instillFormat":"structured/multi_modal_content","items":{"properties":{"image_url":{"properties":{"url":{"description":"Either a URL of the image or the base64 encoded image data.","type":"string"}},"required":["url"],"type":"object"},"text":{"description":"The text content.","instillFormat":"string","type":"string"},"type":{"description":"The type of the content part.","enum":["text","image_url"],"instillFormat":"string","type":"string"}},"required":["type"],"type":"object"},"type":"array"},"role":{"description":"The message role, i.e. 'system', 'user' or 'assistant'","instillFormat":"string","instillUIOrder":0,"title":"Role","type":"string"}},"required":["role","content"],"title":"Chat Message","type":"object"},"title":"Chat history","type":"array"},"frequency_penalty":{"default":0,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n","maximum":2,"minimum":-2,"nullable":true,"type":"number"},"images":{"description":"The images","instillAcceptFormats":["array:image/*"],"instillUIOrder":3,"instillUpstreamTypes":["reference"],"items":{"type":"string"},"title":"Image","type":"array"},"max_tokens":{"description":"The maximum number of [tokens](/tokenizer) to generate in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n","nullable":true,"type":"integer"},"model":{"description":"ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.","enum":["gpt-4-1106-preview","gpt-4-vision-preview","gpt-4","gpt-4-0314","gpt-4-0613","gpt-4-32k","gpt-4-32k-0314","gpt-4-32k-0613","gpt-3.5-turbo","gpt-3.5-turbo-16k","gpt-3.5-turbo-0301","gpt-3.5-turbo-0613","gpt-3.5-turbo-16k-0613"],"example":"gpt-3.5-turbo","type":"string","x-oaiTypeLabel":"string"},"n":{"default":1,"description":"How many chat completion choices to generate for each input message.","example":1,"maximum":128,"minimum":1,"nullable":true,"type":"integer"},"presence_penalty":{"default":0,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n","maximum":2,"minimum":-2,"nullable":true,"type":"number"},"prompt":{"description":"The prompt text","instillAcceptFormats":["string"],"instillUIMultiline":true,"instillUIOrder":1,"instillUpstreamTypes":["value","reference","template"],"title":"Prompt","type":"string"},"response_format":{"description":"An object specifying the format that the model must output. Used to enable JSON mode.","instillUIOrder":8,"properties":{"type":{"default":"text","description":"Setting to `json_object` enables JSON mode. This guarantees that the message the model generates is valid JSON. \n\nNote that your system prompt must still instruct the model to produce JSON, and to help ensure you don't forget, the API will throw an error if the string `JSON` does not appear in your system message. Also note that the message content may be partial (i.e. cut off) if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length. \n\nMust be one of `text` or `json_object`.\n","enum":["text","json_object"],"example":"text","type":"string"}},"required":["type"],"title":"Response Format","type":"object"},"system_message":{"default":"You are a helpful assistant.","description":"The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. By default, the modelâ€™s behavior is using a generic message as \"You are a helpful assistant.\"","instillAcceptFormats":["string"],"instillShortDescription":"The system message helps set the behavior of the assistant","instillUIMultiline":true,"instillUIOrder":2,"instillUpstreamTypes":["value","reference","template"],"title":"System message","type":"string"},"temperature":{"default":1,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n","example":1,"maximum":2,"minimum":0,"nullable":true,"type":"number"},"top_p":{"default":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n","example":1,"maximum":1,"minimum":0,"nullable":true,"type":"number"}},"required":["model","prompt"],"title":"Input","type":"object"}
