{"instillUIOrder":0,"properties":{"chat_history":{"description":"Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should adhere to the format: : {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.","instillAcceptFormats":["structured/chat_messages"],"instillShortDescription":"Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should adhere to the format: : {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.","instillUIOrder":4,"instillUpstreamTypes":["value","reference"],"items":{"$ref":"#/$defs/chat_message"},"title":"Chat history","type":"array"},"frequency_penalty":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/frequency_penalty","instillAcceptFormats":["number","integer"],"instillShortDescription":"Number between -2.0 and 2.0","instillUIOrder":11,"instillUpstreamTypes":["value","reference","template"],"title":"Frequency Penalty"},"images":{"description":"The images","instillAcceptFormats":["array:image/*"],"instillUIOrder":3,"instillUpstreamTypes":["reference"],"items":{"type":"string"},"title":"Image","type":"array"},"max_tokens":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/max_tokens","instillAcceptFormats":["integer"],"instillShortDescription":"The maximum number of <a href=\"https://platform.openai.com/tokenizer\">tokens</a> to generate in the chat completion.","instillUIOrder":7,"instillUpstreamTypes":["value","reference"],"title":"Max Tokens"},"model":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/model","instillAcceptFormats":["string"],"instillShortDescription":"ID of the model to use","instillUIOrder":0,"instillUpstreamTypes":["value","reference","template"],"title":"Model"},"n":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/n","instillAcceptFormats":["integer"],"instillUIOrder":6,"instillUpstreamTypes":["value","reference"],"title":"N"},"presence_penalty":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/presence_penalty","instillAcceptFormats":["number","integer"],"instillShortDescription":"Number between -2.0 and 2.0","instillUIOrder":10,"instillUpstreamTypes":["value","reference","template"],"title":"Presence Penalty"},"prompt":{"description":"The prompt text","instillAcceptFormats":["string"],"instillUIMultiline":true,"instillUIOrder":1,"instillUpstreamTypes":["value","reference","template"],"title":"Prompt","type":"string"},"response_format":{"description":"An object specifying the format that the model must output. Used to enable JSON mode.","instillUIOrder":8,"properties":{"type":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/response_format/properties/type","instillAcceptFormats":["string"],"instillShortDescription":"Setting to `json_object` enables JSON mode. ","instillUIOrder":0,"instillUpstreamTypes":["value","reference"]}},"required":["type"],"title":"Response Format","type":"object"},"system_message":{"default":"You are a helpful assistant.","description":"The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. By default, the modelâ€™s behavior is using a generic message as \"You are a helpful assistant.\"","instillAcceptFormats":["string"],"instillShortDescription":"The system message helps set the behavior of the assistant","instillUIMultiline":true,"instillUIOrder":2,"instillUpstreamTypes":["value","reference","template"],"title":"System message","type":"string"},"temperature":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/temperature","instillAcceptFormats":["number","integer"],"instillShortDescription":"What sampling temperature to use, between 0 and 2.","instillUIOrder":5,"instillUpstreamTypes":["value","reference"],"title":"Temperature"},"top_p":{"$ref":"openai.json#/components/schemas/CreateChatCompletionRequest/properties/top_p","instillAcceptFormats":["number","integer"],"instillShortDescription":"An alternative to sampling with temperature, called nucleus sampling","instillUIOrder":9,"instillUpstreamTypes":["value","reference"],"title":"Top P"}},"required":["model","prompt"],"title":"Input","type":"object","$defs":{"chat_message":{"properties":{"content":{"$ref":"https://raw.githubusercontent.com/instill-ai/component/5e43199713238124565c596433927ef7cb92c565/schema.json#/$defs/instill_types/multi_modal_content","description":"The message content","instillUIOrder":1,"title":"Content"},"role":{"description":"The message role, i.e. 'system', 'user' or 'assistant'","instillFormat":"string","instillUIOrder":0,"title":"Role","type":"string"}},"required":["role","content"],"title":"Chat Message","type":"object"}}}
