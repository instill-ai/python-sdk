{"description":"Input","instillEditOnNodeFields":["text"],"instillUIOrder":0,"properties":{"chunk_token_size":{"default":500,"description":"Number of tokens per text chunk","instillAcceptFormats":["integer"],"instillUIOrder":2,"instillUpstreamTypes":["value","reference"],"minimum":1,"title":"Chunk Token Size","type":"integer"},"model":{"description":"ID of the model to use for tokenization","enum":["gpt-4","gpt-3.5-turbo","text-davinci-003","text-davinci-002","text-davinci-001","text-curie-001","text-babbage-001","text-ada-001","davinci","curie","babbage","ada","code-davinci-002","code-davinci-001","code-cushman-002","code-cushman-001","davinci-codex","cushman-codex","text-davinci-edit-001","code-davinci-edit-001","text-embedding-ada-002","text-similarity-davinci-001","text-similarity-curie-001","text-similarity-babbage-001","text-similarity-ada-001","text-search-davinci-doc-001","text-search-curie-doc-001","text-search-babbage-doc-001","text-search-ada-doc-001","code-search-babbage-code-001","code-search-ada-code-001","gpt2"],"instillAcceptFormats":["string"],"instillUIOrder":1,"instillUpstreamTypes":["value","reference","template"],"title":"Model","type":"string"},"text":{"description":"Text to be split","instillAcceptFormats":["string"],"instillUIMultiline":true,"instillUIOrder":0,"instillUpstreamTypes":["value","reference","template"],"title":"Text","type":"string"}},"required":["text","model"],"title":"Input","type":"object"}
