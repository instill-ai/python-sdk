# syntax=docker/dockerfile:1.7-labs

ARG RAY_VERSION=2.47.0
ARG PYTHON_VERSION=-py312
ARG CUDA_VERSION=-cu128
ARG DEVICE_TYPE=-cpu

#####################################################################################################
# CPU Base Image (amd46)
#####################################################################################################
FROM rayproject/ray:${RAY_VERSION}${PYTHON_VERSION}${CUDA_VERSION} AS base-cpu-amd64

RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    sudo apt-get update -y \
    && sudo apt-get install -y curl ccache git numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \
    && sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1 \
    && sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

# tcmalloc provides better memory allocation efficiency, e.g., holding memory in caches to speed up access of commonly-used objects.
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install py-cpuinfo  # Use this to gather CPU info and optimize based on x86_64 cores

# Set LD_PRELOAD for tcmalloc on x86_64 (different path than ARM)
ENV LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4"

RUN echo 'ulimit -c 0' >> ~/.bashrc

WORKDIR /workspace

# Clone vLLM repository (cache git objects)
ARG VLLM_VERSION=0.9.1
RUN git clone -b v${VLLM_VERSION} https://github.com/vllm-project/vllm

WORKDIR /workspace/vllm

ARG PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"
ENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip install -r requirements/build.txt

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -v -r requirements/common.txt -r requirements/cpu.txt

ARG GIT_REPO_CHECK=0
RUN if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh ; fi

# Enable AVX512 optimizations for x86_64 by default (can be disabled via build arg)
ARG VLLM_CPU_DISABLE_AVX512="false"
ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}

RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/ccache \    
    VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \
    pip install dist/*.whl && \
    rm -rf dist

#####################################################################################################
# CPU Base Image (arm64)
#####################################################################################################
FROM rayproject/ray:${RAY_VERSION}${PYTHON_VERSION}${CUDA_VERSION} AS base-cpu-arm64

RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    sudo apt-get update -y \
    && sudo apt-get install -y curl ccache git numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \
    && sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1 \
    && sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12

# tcmalloc provides better memory allocation efficiency, e.g., holding memory in caches to speed up access of commonly-used objects.
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install py-cpuinfo  # Use this to gather CPU info and optimize based on ARM Neoverse cores

# Set LD_PRELOAD for tcmalloc on ARM
ENV LD_PRELOAD="/usr/lib/aarch64-linux-gnu/libtcmalloc_minimal.so.4"

RUN echo 'ulimit -c 0' >> ~/.bashrc

WORKDIR /workspace

# Clone vLLM repository (cache git objects)
ARG VLLM_VERSION=0.9.1
RUN git clone -b v${VLLM_VERSION} https://github.com/vllm-project/vllm

WORKDIR /workspace/vllm

ARG PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"
ENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip install -r requirements/build.txt

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -v -r requirements/common.txt -r requirements/cpu.txt

ARG GIT_REPO_CHECK=0
RUN if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh ; fi

# Disabling AVX512 specific optimizations for ARM
ARG VLLM_CPU_DISABLE_AVX512="true"
ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}

RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/ccache \    
    VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \
    pip install dist/*.whl && \
    rm -rf dist

#####################################################################################################
# GPU Base Image (amd64)
#####################################################################################################
FROM rayproject/ray:${RAY_VERSION}${PYTHON_VERSION}${CUDA_VERSION} AS base-gpu-amd64

# Install vLLM with pre-built wheels supporting CUDA 12.2-12.8
# The wheel will automatically detect the appropriate CUDA version at runtime
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip install vllm[all] --extra-index-url https://download.pytorch.org/whl${CUDA_VERSION/-//}

# Verify vLLM installation and CUDA support
RUN python -c "import vllm; print(f'vLLM version: {vllm.__version__}'); print('CUDA available:', vllm.utils.cuda_utils.is_cuda_available())"

###############################################################################
# Main Image
###############################################################################
FROM base${DEVICE_TYPE}-${TARGETARCH}

WORKDIR /home/ray

# Install base packages
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    sudo apt-get update && \
    DEBIAN_FRONTEND=noninteractive sudo apt-get install -y curl vim && \
    sudo rm -rf /var/lib/apt/lists/*

# Install system packages
ARG SYSTEM_PACKAGES
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    sudo apt-get update && \
    for package in ${SYSTEM_PACKAGES}; do \
    DEBIAN_FRONTEND=noninteractive sudo apt-get install -y $package; \
    done && \
    sudo rm -rf /var/lib/apt/lists/*

# Install python packages
ARG PYTHON_PACKAGES
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    for package in ${PYTHON_PACKAGES}; do \
    pip install --default-timeout=1000 $package; \
    done

# The python-sdk is a local module that is not installed in the Ray worker environment.
# Ray workers need to deserialize model configurations and deployment settings that were
# created in the runtime environment. These objects may depend on this python-sdk.
# Instead of installing the SDK in the Ray image, we copy
# the entire SDK to /home/ray in development mode (editable or wheel) or
# install the SDK in the Ray image in production mode
COPY --chown=ray:users . .

ARG INSTILL_PYTHON_SDK_PROJECT_NAME
ARG INSTILL_PYTHON_SDK_VERSION
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \    
    if [ ! -z "$INSTILL_PYTHON_SDK_PROJECT_NAME" ]; then \
    pip install -e ${INSTILL_PYTHON_SDK_PROJECT_NAME}; \
    elif [ -f instill_sdk-${INSTILL_PYTHON_SDK_VERSION}dev-py3-none-any.whl ]; then \
    pip install instill_sdk-${INSTILL_PYTHON_SDK_VERSION}dev-py3-none-any.whl; \
    else \
    pip install --default-timeout=1000 instill-sdk==${INSTILL_PYTHON_SDK_VERSION}; \
    fi;

USER ray

# The python-sdk is a local module that is not installed in the Ray worker environment.
# Ray workers need to deserialize model configurations and deployment settings that were
# created in the runtime environment. These objects may depend on this python-sdk.
# Instead of installing the SDK in the Ray image, we copy
# the entire SDK to /home/ray and set PYTHONPATH to allow dynamic importing.
ARG PYTHONPATH_USER_DEFINED_PROTO
ENV PYTHONPATH=${PYTHONPATH_USER_DEFINED_PROTO}
