{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SDK tool provides some helper functions to allow you to create and deploy custom models with ease\n",
    "\n",
    "Let's say we want to serve a [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo) with [Instill Model](https://github.com/instill-ai/model)\n",
    "\n",
    "1. First we need to create a file structure like the following\n",
    "\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â””â”€â”€ sd_turbo                <=== your model name\n",
    "    â””â”€â”€ 1                   <=== your model version\n",
    "        â”œâ”€â”€ model.py        <=== your model file\n",
    "        â”œâ”€â”€ ray_pb2.py\n",
    "        â”œâ”€â”€ ray_pb2.pyi\n",
    "        â”œâ”€â”€ ray_pb2_grpc.py\n",
    "        â””â”€â”€ sd_turbo        <=== model weights and dependecy folder clone from huggingface (remember to follow the LICENSE of each model)\n",
    "```\n",
    "\n",
    "Within the `README.md` you will have to put in the info about the model inbetween the `---` section, and a brief intro down below. For example\n",
    "```\n",
    "---\n",
    "Task: TextToImage\n",
    "Tags:\n",
    "  - TextToImage\n",
    "  - Text-To-Image\n",
    "  - Diffusion\n",
    "---\n",
    "\n",
    "# Model-SD-Turbo\n",
    "\n",
    "ðŸ”¥ðŸ”¥ðŸ”¥ Deploy [Stable Diffusion Turbo](https://huggingface.co/stabilityai/sd-turbo)\n",
    "\n",
    "```\n",
    "\n",
    "2. Then we put the 3 proto definition files inside the `./{model_name}/{version}` folder, you can find them [here](https://github.com/instill-ai/model-backend/tree/main/assets/ray/proto), we are working to avoid this step in the future.\n",
    "3. Now we can `git clone` the dependencies from huggingface, with git lfs.\n",
    "```\n",
    "git lfs install\n",
    "git clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6 tinyllama\n",
    "```\n",
    "4. Next, we start writting our model file, which with the help of the SDK, is relatively similar to what you would expect when developing in your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary packages\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import ray\n",
    "from ray import serve\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# import SDK helper functions\n",
    "# const package hosts the standard Datatypes and Input class for each standard Instill AI Tasks\n",
    "from instill.helpers.const import DataType, TextToImageInput\n",
    "# ray_io package hosts the parsers to easily convert request payload into input paramaters, and model outputs to response\n",
    "from instill.helpers.ray_io import StandardTaskIO\n",
    "# ray_config package hosts the config for the model resource\n",
    "from instill.helpers.ray_config import (\n",
    "    InstillRayModelConfig,\n",
    "    entry,\n",
    ")\n",
    "# ray_pb2 is the proto definition of the grpc request/response\n",
    "from ray_pb2 import (\n",
    "    ModelReadyRequest,\n",
    "    ModelReadyResponse,\n",
    "    ModelMetadataRequest,\n",
    "    ModelMetadataResponse,\n",
    "    ModelInferRequest,\n",
    "    ModelInferResponse,\n",
    "    InferTensor,\n",
    ")\n",
    "\n",
    "# use Ray's serve.deployment decorator to convert the model class to servable model\n",
    "@serve.deployment()\n",
    "class SDTurbo:\n",
    "\n",
    "    # within the __init__ function, setup the model instance with the desired framework, in this\n",
    "    # case is the pipeline from transformers\n",
    "    def __init__(self, model_path: str):\n",
    "        self.pipeline = DiffusionPipeline.from_pretrained(model_path)\n",
    "\n",
    "    # ModelMetadata tells the server what inputs the model is expecting\n",
    "    # It will be standard for the same AI task\n",
    "    def ModelMetadata(self, req: ModelMetadataRequest) -> ModelMetadataResponse:\n",
    "        resp = ModelMetadataResponse(\n",
    "            name=req.name,\n",
    "            versions=req.version,\n",
    "            framework=\"python\",\n",
    "            inputs=[\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"prompt\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"negative_prompt\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"samples\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"scheduler\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[-1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"steps\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"guidance_scale\",\n",
    "                    datatype=str(DataType.TYPE_FP32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"seed\",\n",
    "                    datatype=str(DataType.TYPE_UINT64.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"extra_params\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "            ],\n",
    "            outputs=[\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"images\",\n",
    "                    datatype=str(DataType.TYPE_FP32.name),\n",
    "                    shape=[-1, -1, -1, -1],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    # ModelReady is the healthcheck method for the server\n",
    "    # implement your own logic and it will reflect on the console\n",
    "    def ModelReady(self, req: ModelReadyRequest) -> ModelReadyResponse:\n",
    "        resp = ModelReadyResponse(ready=True)\n",
    "        return resp\n",
    "\n",
    "    # ModelInfer is the method handling the trigger request from Instill Model\n",
    "    async def ModelInfer(self, request: ModelInferRequest) -> ModelInferResponse:\n",
    "        # prepare the response\n",
    "        resp = ModelInferResponse(\n",
    "            model_name=request.model_name,\n",
    "            model_version=request.model_version,\n",
    "            outputs=[],\n",
    "            raw_output_contents=[],\n",
    "        )\n",
    "\n",
    "        # use StandardTaskIO package to parse the request and get the corresponding input\n",
    "        # for text-to-image task\n",
    "        task_text_to_image_input: TextToImageInput = (\n",
    "            StandardTaskIO.parse_task_text_to_image_input(request=request)\n",
    "        )\n",
    "\n",
    "        # prepare prompt with chat template\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "            task_text_generation_chat_input.conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        # inference\n",
    "        image = self.pipeline(\n",
    "            prompt=task_text_to_image_input.prompt,\n",
    "            negative_prompt=task_text_to_image_input.negative_prompt,\n",
    "            do_sample=True,\n",
    "            num_images_per_prompt=task_text_to_image_input.samples,\n",
    "            guidance_scale=task_text_to_image_input.guidance_scale,\n",
    "            num_inference_steps=task_text_to_image_input.steps,\n",
    "            **task_text_to_image_input.extra_params,\n",
    "        ).images[0]\n",
    "\n",
    "        # convert the output into response output with again the StandardTaskIO\n",
    "        task_text_to_image_output = StandardTaskIO.parse_task_text_to_image_output(\n",
    "            image=image\n",
    "        )\n",
    "\n",
    "        # specify the output dimension\n",
    "        resp.outputs.append(\n",
    "            InferTensor(\n",
    "                name=\"text\",\n",
    "                shape=[1, 1, -1, -1],\n",
    "                datatype=str(DataType.TYPE_FP32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # finally insert the output into the response\n",
    "        resp.raw_output_contents.append(task_text_to_image_output)\n",
    "\n",
    "        return resp\n",
    "\n",
    "# global deploy_model is necessary for server to trigger the deployment\n",
    "# basically nothing needs to be changed here, we are working on to remove\n",
    "# the neccessity of this method as well\n",
    "def deploy_model(model_config: InstillRayModelConfig):\n",
    "    c_app = SDTurbo.options(\n",
    "        name=model_config.application_name,\n",
    "        ray_actor_options=model_config.ray_actor_options,\n",
    "        max_concurrent_queries=model_config.max_concurrent_queries,\n",
    "        autoscaling_config=model_config.ray_autoscaling_options,\n",
    "    ).bind(model_config.model_path)\n",
    "\n",
    "    serve.run(\n",
    "        c_app, name=model_config.model_name, route_prefix=model_config.route_prefix\n",
    "    )\n",
    "\n",
    "# global deploy_model is necessary for server to trigger the undeployment\n",
    "def undeploy_model(model_name: str):\n",
    "    serve.delete(model_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # the value passed into entry() needs to match the dependecy folder/file name\n",
    "    func, model_config = entry(\"sd_turbo\")\n",
    "\n",
    "    ray.init(address=model_config.ray_addr)\n",
    "\n",
    "    # setup how many resources the model needs\n",
    "    # this heavily depends on your machine\n",
    "    # can reference `ray_actor_options` from Ray Serve\n",
    "    model_config.ray_actor_options[\"num_cpus\"] = 6\n",
    "\n",
    "    if func == \"deploy\":\n",
    "        deploy_model(model_config=model_config)\n",
    "    elif func == \"undeploy\":\n",
    "        undeploy_model(model_name=model_config.model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we can pack it up and serve it on `Instill Model`! Simply\n",
    "```bash\n",
    "zip -r \"sd-turbo.zip\" .\n",
    "```\n",
    "Or alternatively, if you have a LFS server or DVC bucket setup somewhere, you can also push the files along with the `.dvc` or lfs files onto github, and use our github import.\n",
    "\n",
    "Now go to `Model Hub` page on Instill console and create a model from local with this zip, and profit!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
