{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SDK tool provides some helper functions to allow you to create and deploy custom models with ease\n",
    "\n",
    "Let's say we want to serve a [Tiny-Llama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6) with [Instill Model](https://github.com/instill-ai/model)\n",
    "\n",
    "1. First we need to create a file structure like the following\n",
    "\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ README.md\n",
    "â””â”€â”€ tiny_llama               <=== your model name\n",
    "    â””â”€â”€ 1                    <=== your model version\n",
    "        â”œâ”€â”€ model.py         <=== your model file\n",
    "        â”œâ”€â”€ ray_pb2.py\n",
    "        â”œâ”€â”€ ray_pb2.pyi\n",
    "        â”œâ”€â”€ ray_pb2_grpc.py\n",
    "        â””â”€â”€ tinyllama        <=== model weights and dependecy folder clone from huggingface (remember to follow the LICENSE of each model)\n",
    "```\n",
    "\n",
    "Within the `README.md` you will have to put in the info about the model inbetween the `---` section, and a brief intro down below. For example\n",
    "```\n",
    "---\n",
    "Task: TextGenerationChat\n",
    "Tags:\n",
    "  - TextGenerationChat\n",
    "  - TinyLlama-1.1B-Chat\n",
    "---\n",
    "\n",
    "# Model-llama2-7b-chat-dvc\n",
    "\n",
    "ðŸ”¥ðŸ”¥ðŸ”¥ Deploy [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6) model.\n",
    "```\n",
    "\n",
    "2. Then we put the 3 proto definition files inside the `./{model_name}/{version}` folder, you can find them [here](https://github.com/instill-ai/model-backend/tree/main/assets/ray/proto), we are working to avoid this step in the future.\n",
    "3. Now we can `git clone` the dependencies from huggingface, with git lfs.\n",
    "```\n",
    "git lfs install\n",
    "git clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6 $PROJECT_ROOT/{modelname}/{version}/tinyllama\n",
    "```\n",
    "4. Next, we start writting our model file, which with the help of the SDK, is relatively similar to what you would expect when developing in your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary packages\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# import SDK helper functions\n",
    "# const package hosts the standard Datatypes and Input class for each standard Instill AI Tasks\n",
    "from instill.helpers.const import DataType, TextGenerationChatInput\n",
    "# ray_io package hosts the parsers to easily convert request payload into input paramaters, and model outputs to response\n",
    "from instill.helpers.ray_io import StandardTaskIO\n",
    "# ray_config package hosts the decorators and deployment object for model class\n",
    "from instill.helpers.ray_config import instill_deployment, InstillDeployable\n",
    "# ray_pb2 is the proto definition of the grpc request/response\n",
    "from ray_pb2 import (\n",
    "    ModelReadyRequest,\n",
    "    ModelReadyResponse,\n",
    "    ModelMetadataRequest,\n",
    "    ModelMetadataResponse,\n",
    "    ModelInferRequest,\n",
    "    ModelInferResponse,\n",
    "    InferTensor,\n",
    ")\n",
    "\n",
    "# use instill_deployment decorator to convert the model class to servable model\n",
    "@instill_deployment\n",
    "class TinyLlama:\n",
    "\n",
    "    # within the __init__ function, setup the model instance with the desired framework, in this\n",
    "    # case is the pipeline from transformers\n",
    "    def __init__(self, model_path: str):\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_path,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=\"cpu\",\n",
    "        )\n",
    "\n",
    "    # ModelMetadata tells the server what inputs and outputs the model is expecting\n",
    "    def ModelMetadata(self, req: ModelMetadataRequest) -> ModelMetadataResponse:\n",
    "        resp = ModelMetadataResponse(\n",
    "            name=req.name,\n",
    "            versions=req.version,\n",
    "            framework=\"python\",\n",
    "            inputs=[\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"conversation\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"max_new_tokens\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"temperature\",\n",
    "                    datatype=str(DataType.TYPE_FP32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"top_k\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"random_seed\",\n",
    "                    datatype=str(DataType.TYPE_UINT64.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"extra_params\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "            ],\n",
    "            outputs=[\n",
    "                ModelMetadataResponse.TensorMetadata(\n",
    "                    name=\"text\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[-1, -1],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    # ModelReady is the healthcheck method for the server\n",
    "    # implement your own logic and it will reflect on the console\n",
    "    def ModelReady(self, req: ModelReadyRequest) -> ModelReadyResponse:\n",
    "        resp = ModelReadyResponse(ready=True)\n",
    "        return resp\n",
    "\n",
    "    # ModelInfer is the method handling the trigger request from Instill Model\n",
    "    async def ModelInfer(self, request: ModelInferRequest) -> ModelInferResponse:\n",
    "        # prepare the response\n",
    "        resp = ModelInferResponse(\n",
    "            model_name=request.model_name,\n",
    "            model_version=request.model_version,\n",
    "            outputs=[],\n",
    "            raw_output_contents=[],\n",
    "        )\n",
    "\n",
    "        # use StandardTaskIO package to parse the request and get the corresponding input\n",
    "        # for text-generation-chat task\n",
    "        task_text_generation_chat_input: TextGenerationChatInput = (\n",
    "            StandardTaskIO.parse_task_text_generation_chat_input(request=request)\n",
    "        )\n",
    "\n",
    "        # prepare prompt with chat template\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "            task_text_generation_chat_input.conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        # inference\n",
    "        sequences = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=task_text_generation_chat_input.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=task_text_generation_chat_input.temperature,\n",
    "            top_k=task_text_generation_chat_input.top_k,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "        # convert the output into response output with again the StandardTaskIO\n",
    "        task_text_generation_chat_output = (\n",
    "            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)\n",
    "        )\n",
    "\n",
    "        # specify the output dimension\n",
    "        resp.outputs.append(\n",
    "            InferTensor(\n",
    "                name=\"text\",\n",
    "                shape=[1, len(sequences)],\n",
    "                datatype=str(DataType.TYPE_STRING),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # finally insert the output into the response\n",
    "        resp.raw_output_contents.append(task_text_generation_chat_output)\n",
    "\n",
    "        return resp\n",
    "\n",
    "# now simply declare a global deployable instance with model weight name or model file name\n",
    "# and specify if this model is going to use GPU or not\n",
    "deployable = InstillDeployable(TinyLlama, model_weight_or_folder_name=\"tinyllama\", use_gpu=True)\n",
    "\n",
    "# you can also have a fine-grained control of the min/max replica numbers\n",
    "deployable.update_max_replicas(2)\n",
    "deployable.update_min_replicas(0)\n",
    "\n",
    "# we plan to open up more detailed resource control in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we can pack it up and serve it on `Instill Model`! Simply\n",
    "```bash\n",
    "zip -r \"tiny-llama.zip\" .\n",
    "```\n",
    "Or alternatively, if you have a LFS server or DVC bucket setup somewhere, you can also push the files along with the `.dvc` or lfs files onto github, and use our github import.\n",
    "\n",
    "Now go to `Model Hub` page on Instill console and create a model from local with this zip, and profit!\n",
    "\n",
    "Here is a sample request and response with this model\n",
    "\n",
    "_*req:*_\n",
    "```bash\n",
    "curl --location 'http://localhost:8080/model/v1alpha/users/admin/models/tinyllama/trigger' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--header 'Authorization: Bearer instill_sk_***' \\\n",
    "--data '{\n",
    "    \"task_inputs\": [\n",
    "        {\n",
    "            \"text_generation_chat\": {\n",
    "                \"conversation\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"is it unhealthy to stay up late?\"\n",
    "                    }\n",
    "                ],\n",
    "                \"top_k\": 5,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}'\n",
    "```\n",
    "_*resp:*_\n",
    "```json\n",
    "{\n",
    "    \"task\": \"TASK_TEXT_GENERATION_CHAT\",\n",
    "    \"task_outputs\": [\n",
    "        {\n",
    "            \"text_generation\": {\n",
    "                \"text\": \"<|user|>\\nis it unhealthy to stay up late?</s>\\n<|assistant|>\\nYes, staying up late can be unhealthy. Longer hours of sleep are important for good health and well-being. The body needs time to rest and recover after a long day, and excessive sleep can lead to a range of health problems, including insomnia, obesity, and heart disease. It's essential to set a regular sleep schedule, limit screen time before bedtime, and get enough sleep to avoid sleep-related health issues.\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
