{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SDK tool provides some helper functions to allow you to create and deploy custom models with ease\n",
    "\n",
    "Let's say we want to serve a [Tiny-Llama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) with [Instill Model](https://github.com/instill-ai/model)\n",
    "\n",
    "1. First we need to create a file structure like the following\n",
    "\n",
    "```bash\n",
    ".                    <=== your model folder\n",
    "â”œâ”€â”€ README.md        <=== your model README\n",
    "â”œâ”€â”€ model.py         <=== your model file\n",
    "â””â”€â”€ tinyllama        <=== model weights and dependecy folder clone from huggingface (remember to follow the LICENSE of each model)\n",
    "```\n",
    "\n",
    "Within the `README.md` you will have to put in the info about the model in-between the `---` section, and a brief intro down below. For example\n",
    "```\n",
    "---\n",
    "Task: TextGenerationChat\n",
    "Tags:\n",
    "  - TextGenerationChat\n",
    "  - TinyLlama-1.1B-Chat\n",
    "---\n",
    "\n",
    "Learn more about it [here](https://www.instill.tech/docs/latest/model/prepare#model-card-metadata)\n",
    "\n",
    "# Model-TinyLlama-1.1b-chat-dvc\n",
    "\n",
    "ðŸ”¥ðŸ”¥ðŸ”¥ Deploy [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) model.\n",
    "```\n",
    "\n",
    "2. Now we can `git clone` the dependencies from huggingface, with git lfs.\n",
    "```\n",
    "git lfs install\n",
    "git clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0 $PROJECT_ROOT/{modelname}/{version}/tinyllama\n",
    "```\n",
    "3. Next, we start writing our model file, which with the help of the SDK, is relatively similar to what you would expect when developing in your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary packages\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# import SDK helper functions\n",
    "# const package hosts the standard Datatypes and Input class for each standard Instill AI Tasks\n",
    "from instill.helpers.const import DataType, TextGenerationChatInput\n",
    "\n",
    "# ray_io package hosts the parsers to easily convert request payload into input paramaters, and model outputs to response\n",
    "from instill.helpers.ray_io import StandardTaskIO\n",
    "\n",
    "# ray_config package hosts the decorators and deployment object for model class\n",
    "from instill.helpers.ray_config import instill_deployment, InstillDeployable\n",
    "from instill.helpers import (\n",
    "    construct_infer_response,\n",
    "    construct_metadata_response,\n",
    "    Metadata,\n",
    ")\n",
    "\n",
    "\n",
    "# use instill_deployment decorator to convert the model class to servable model\n",
    "@instill_deployment\n",
    "class TinyLlama:\n",
    "    # within the __init__ function, setup the model instance with the desired framework, in this\n",
    "    # case is the pipeline from transformers\n",
    "    def __init__(self):\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"tinyllama\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    # ModelMetadata tells the server what inputs and outputs the model is expecting\n",
    "    def ModelMetadata(self, req):\n",
    "        resp = construct_metadata_response(\n",
    "            req=req,\n",
    "            inputs=[\n",
    "                Metadata(\n",
    "                    name=\"prompt\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"prompt_images\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"chat_history\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"system_message\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"max_new_tokens\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"temperature\",\n",
    "                    datatype=str(DataType.TYPE_FP32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"top_k\",\n",
    "                    datatype=str(DataType.TYPE_UINT32.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"seed\",\n",
    "                    datatype=str(DataType.TYPE_UINT64.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "                Metadata(\n",
    "                    name=\"extra_params\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[1],\n",
    "                ),\n",
    "            ],\n",
    "            outputs=[\n",
    "                Metadata(\n",
    "                    name=\"text\",\n",
    "                    datatype=str(DataType.TYPE_STRING.name),\n",
    "                    shape=[-1, -1],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    # ModelInfer is the method handling the trigger request from Instill Model\n",
    "    async def __call__(self, request):\n",
    "        resp_outputs = []\n",
    "        resp_raw_outputs = []\n",
    "\n",
    "        # use StandardTaskIO package to parse the request and get the corresponding input\n",
    "        # for text-generation-chat task\n",
    "        task_text_generation_chat_input: TextGenerationChatInput = (\n",
    "            StandardTaskIO.parse_task_text_generation_chat_input(request=request)\n",
    "        )\n",
    "\n",
    "        # prepare prompt with chat template\n",
    "        prompt = self.pipeline.tokenizer.apply_chat_template(\n",
    "            task_text_generation_chat_input.chat_history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        # inference\n",
    "        sequences = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=task_text_generation_chat_input.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=task_text_generation_chat_input.temperature,\n",
    "            top_k=task_text_generation_chat_input.top_k,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "        # convert the output into response output with again the StandardTaskIO\n",
    "        task_text_generation_chat_output = (\n",
    "            StandardTaskIO.parse_task_text_generation_chat_output(sequences=sequences)\n",
    "        )\n",
    "\n",
    "        # specify the output dimension\n",
    "        resp_outputs.append(\n",
    "            Metadata(\n",
    "                name=\"text\",\n",
    "                shape=[1, len(sequences)],\n",
    "                datatype=str(DataType.TYPE_STRING),\n",
    "            )\n",
    "        )\n",
    "        # finally insert the output into the response\n",
    "        resp_raw_outputs.append(task_text_generation_chat_output)\n",
    "\n",
    "        return construct_infer_response(\n",
    "            req=request,\n",
    "            outputs=resp_outputs,\n",
    "            raw_outputs=resp_raw_outputs,\n",
    "        )\n",
    "\n",
    "\n",
    "# now simply declare a global deployable instance with model weight name or model file name\n",
    "# and specify if this model is going to use GPU or not\n",
    "deployable = InstillDeployable(\n",
    "    TinyLlama, model_weight_or_folder_name=\"tinyllama\", use_gpu=True\n",
    ")\n",
    "\n",
    "# you can also have a fine-grained control of the min/max replica numbers\n",
    "deployable.update_max_replicas(2)\n",
    "deployable.update_min_replicas(0)\n",
    "\n",
    "# we plan to open up more detailed resource control in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we can pack it up and serve it on `Instill Core`! Simply\n",
    "```bash\n",
    "zip -r \"tiny-llama.zip\" .\n",
    "```\n",
    "Or alternatively, if you have a LFS server or DVC bucket setup somewhere, you can also push the files along with the `.dvc` or lfs files onto github, and use our github import.\n",
    "\n",
    "Now go to `Model` page on Instill console and create a model from local with this zip, and profit!\n",
    "\n",
    "Here is a sample request and response with this model\n",
    "\n",
    "_req:_\n",
    "```bash\n",
    "curl --location 'http://localhost:8080/model/v1alpha/users/admin/models/tinyllama/trigger' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--header 'Authorization: Bearer instill_sk_***' \\\n",
    "--data '{\n",
    "    \"task_inputs\": [\n",
    "        {\n",
    "            \"text_generation_chat\": {\n",
    "                \"conversation\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"is it unhealthy to stay up late?\"\n",
    "                    }\n",
    "                ],\n",
    "                \"top_k\": 5,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}'\n",
    "```\n",
    "_resp:_\n",
    "```json\n",
    "{\n",
    "    \"task\": \"TASK_TEXT_GENERATION_CHAT\",\n",
    "    \"task_outputs\": [\n",
    "        {\n",
    "            \"text_generation\": {\n",
    "                \"text\": \"<|user|>\\nis it unhealthy to stay up late?</s>\\n<|assistant|>\\nYes, staying up late can be unhealthy. Longer hours of sleep are important for good health and well-being. The body needs time to rest and recover after a long day, and excessive sleep can lead to a range of health problems, including insomnia, obesity, and heart disease. It's essential to set a regular sleep schedule, limit screen time before bedtime, and get enough sleep to avoid sleep-related health issues.\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
